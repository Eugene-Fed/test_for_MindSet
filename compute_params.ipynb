{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f99783ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from models.ssd_model import build_model\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59fa9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 32 # Height of the input images\n",
    "img_width = 32 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "n_classes = 76 # Number of positive classes\n",
    "\n",
    "intensity_mean = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "intensity_range = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "# scales = [1.5/52, 1.5/26, 1.5/18, 1.5/15]\n",
    "scales = [0.05, 0.08, 0.12, 0.2] # An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
    "# aspect_ratios = [0.5, 1.0, 2.0] # The list of aspect ratios for the anchor boxes\n",
    "aspect_ratios = [0.5, 1.0, 2]\n",
    "two_boxes_for_ar1 = True # Whether or not you want to generate two anchor boxes for aspect ratio 1\n",
    "steps = None # In case you'd like to set the step sizes for the anchor box grids manually; not recommended\n",
    "offsets = None # In case you'd like to set the offsets for the anchor box grids manually; not recommended\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [1.0, 1.0, 1.0, 1.0] # The list of variances by which the encoded target coordinates are scaled\n",
    "normalize_coords = True # Whether or not the model is supposed to use coordinates relative to the image size\n",
    "predicted_layers = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c9ac968",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " identity_layer (Lambda)        (None, 32, 32, 3)    0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_mean_normalization (Lamb  (None, 32, 32, 3)   0           ['identity_layer[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " input_stddev_normalization (La  (None, 32, 32, 3)   0           ['input_mean_normalization[0][0]'\n",
      " mbda)                                                           ]                                \n",
      "                                                                                                  \n",
      " model (Functional)             [(None, 4, 4, 96),   939120      ['input_stddev_normalization[0][0\n",
      "                                 (None, 2, 2, 288),              ]']                              \n",
      "                                 (None, 1, 1, 576)]                                               \n",
      "                                                                                                  \n",
      " f_map3 (SeparableConv2D)       (None, 1, 1, 128)    79040       ['model[0][2]']                  \n",
      "                                                                                                  \n",
      " f_map2 (SeparableConv2D)       (None, 2, 2, 128)    39584       ['model[0][1]']                  \n",
      "                                                                                                  \n",
      " up3 (UpSampling2D)             (None, 2, 2, 128)    0           ['f_map3[0][0]']                 \n",
      "                                                                                                  \n",
      " concat23 (Concatenate)         (None, 2, 2, 256)    0           ['f_map2[0][0]',                 \n",
      "                                                                  'up3[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2_out (Conv2D)             (None, 2, 2, 128)    131200      ['concat23[0][0]']               \n",
      "                                                                                                  \n",
      " f_map1_1 (SeparableConv2D)     (None, 4, 4, 128)    13280       ['model[0][0]']                  \n",
      "                                                                                                  \n",
      " up2 (UpSampling2D)             (None, 4, 4, 128)    0           ['conv2_out[0][0]']              \n",
      "                                                                                                  \n",
      " concat12 (Concatenate)         (None, 4, 4, 256)    0           ['f_map1_1[0][0]',               \n",
      "                                                                  'up2[0][0]']                    \n",
      "                                                                                                  \n",
      " out1 (Conv2D)                  (None, 4, 4, 128)    131200      ['concat12[0][0]']               \n",
      "                                                                                                  \n",
      " classes1 (Conv2D)              (None, 4, 4, 308)    355124      ['out1[0][0]']                   \n",
      "                                                                                                  \n",
      " classes2 (Conv2D)              (None, 2, 2, 308)    355124      ['conv2_out[0][0]']              \n",
      "                                                                                                  \n",
      " classes3 (Conv2D)              (None, 1, 1, 308)    355124      ['f_map3[0][0]']                 \n",
      "                                                                                                  \n",
      " boxes1 (Conv2D)                (None, 4, 4, 16)     18448       ['out1[0][0]']                   \n",
      "                                                                                                  \n",
      " boxes2 (Conv2D)                (None, 2, 2, 16)     18448       ['conv2_out[0][0]']              \n",
      "                                                                                                  \n",
      " boxes3 (Conv2D)                (None, 1, 1, 16)     18448       ['f_map3[0][0]']                 \n",
      "                                                                                                  \n",
      " classes1_reshape (Reshape)     (None, 64, 77)       0           ['classes1[0][0]']               \n",
      "                                                                                                  \n",
      " classes2_reshape (Reshape)     (None, 16, 77)       0           ['classes2[0][0]']               \n",
      "                                                                                                  \n",
      " classes3_reshape (Reshape)     (None, 4, 77)        0           ['classes3[0][0]']               \n",
      "                                                                                                  \n",
      " anchors1 (AnchorBoxes)         (None, 4, 4, 4, 8)   0           ['boxes1[0][0]']                 \n",
      "                                                                                                  \n",
      " anchors2 (AnchorBoxes)         (None, 2, 2, 4, 8)   0           ['boxes2[0][0]']                 \n",
      "                                                                                                  \n",
      " anchors3 (AnchorBoxes)         (None, 1, 1, 4, 8)   0           ['boxes3[0][0]']                 \n",
      "                                                                                                  \n",
      " classes_concat (Concatenate)   (None, 84, 77)       0           ['classes1_reshape[0][0]',       \n",
      "                                                                  'classes2_reshape[0][0]',       \n",
      "                                                                  'classes3_reshape[0][0]']       \n",
      "                                                                                                  \n",
      " boxes1_reshape (Reshape)       (None, 64, 4)        0           ['boxes1[0][0]']                 \n",
      "                                                                                                  \n",
      " boxes2_reshape (Reshape)       (None, 16, 4)        0           ['boxes2[0][0]']                 \n",
      "                                                                                                  \n",
      " boxes3_reshape (Reshape)       (None, 4, 4)         0           ['boxes3[0][0]']                 \n",
      "                                                                                                  \n",
      " anchors1_reshape (Reshape)     (None, 64, 8)        0           ['anchors1[0][0]']               \n",
      "                                                                                                  \n",
      " anchors2_reshape (Reshape)     (None, 16, 8)        0           ['anchors2[0][0]']               \n",
      "                                                                                                  \n",
      " anchors3_reshape (Reshape)     (None, 4, 8)         0           ['anchors3[0][0]']               \n",
      "                                                                                                  \n",
      " classes_softmax (Activation)   (None, 84, 77)       0           ['classes_concat[0][0]']         \n",
      "                                                                                                  \n",
      " boxes_concat (Concatenate)     (None, 84, 4)        0           ['boxes1_reshape[0][0]',         \n",
      "                                                                  'boxes2_reshape[0][0]',         \n",
      "                                                                  'boxes3_reshape[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " anchors_concat (Concatenate)   (None, 84, 8)        0           ['anchors1_reshape[0][0]',       \n",
      "                                                                  'anchors2_reshape[0][0]',       \n",
      "                                                                  'anchors3_reshape[0][0]']       \n",
      "                                                                                                  \n",
      " predictions (Concatenate)      (None, 84, 89)       0           ['classes_softmax[0][0]',        \n",
      "                                                                  'boxes_concat[0][0]',           \n",
      "                                                                  'anchors_concat[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,454,140\n",
      "Trainable params: 2,442,028\n",
      "Non-trainable params: 12,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_global=aspect_ratios,\n",
    "                    aspect_ratios_per_layer=None,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=intensity_mean,\n",
    "                    divide_by_stddev=intensity_range)\n",
    "\n",
    "# 2: Optional: Load some weights\n",
    "\n",
    "# model.load_weights('ssd_mobnet\\ssd3_epoch-02_loss-1.4484.h5', by_name=True)\n",
    "\n",
    "# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20491ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "# model_path = 'ssd_for_object/3stola288_[0.05, 0.08, 0.12, 0.2]_[0.7, 1.0, 1.3].h5'\n",
    "\n",
    "# # We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "# ssd_loss = SSDLoss(neg_pos_ratio=1, alpha=0.1)\n",
    "\n",
    "# K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "# model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "#                                                'compute_loss': ssd_loss.compute_loss})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57b3e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_wh_to_xy(boxes1)\n",
    "    boxes2_corners = convert_wh_to_xy(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
    "\n",
    "def match_iou(gt_boxes, pred_boxes):\n",
    "    iou_matrix = compute_iou(gt_boxes, pred_boxes)\n",
    "#     print(iou_matrix)\n",
    "    max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "    positive_mask = tf.greater_equal(max_iou, 0.4)\n",
    "\n",
    "    return max_iou, positive_mask\n",
    "\n",
    "def convert_wh_to_xy(boxes):\n",
    "    convert_boxes = np.empty_like(boxes)\n",
    "    convert_boxes[:,[0,1]] = boxes[:,[0,1]] - boxes[:,[2,3]]/2\n",
    "    convert_boxes[:,[2,3]] = boxes[:,[0,1]] + boxes[:,[2,3]]/2\n",
    "    convert_boxes[convert_boxes<0] = 0.0\n",
    "    return convert_boxes\n",
    "\n",
    "def convert_xy_to_wh(boxes):\n",
    "    convert_boxes = np.empty_like(boxes)\n",
    "    convert_boxes[:,[2,3]] = boxes[:,[2,3]] - boxes[:,[0,1]]\n",
    "    convert_boxes[:,[0,1]] = (boxes[:,[0,1]] + boxes[:,[2,3]])/2\n",
    "    return convert_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4333e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_sizes = [model.get_layer('classes1').output_shape[1:3],\n",
    "                   model.get_layer('classes2').output_shape[1:3],\n",
    "                  model.get_layer('classes3').output_shape[1:3]]\n",
    "\n",
    "def get_anchors(scales, aspect_ratios, n_predictor_layers = 3, coords='centroids'):\n",
    "    anchor_boxes_wh = []\n",
    "    \n",
    "    steps = [None] * n_predictor_layers\n",
    "    offsets = [None] * n_predictor_layers\n",
    "\n",
    "#     print(scales)\n",
    "    for i, size in enumerate(predictor_sizes):\n",
    "        \n",
    "        x = np.zeros((1,) + size + (3,))\n",
    "        anchors = AnchorBoxes(img_height,\n",
    "                          img_width,\n",
    "                          this_scale=scales[i],\n",
    "                          next_scale=scales[i+1],\n",
    "                          aspect_ratios=aspect_ratios[i],\n",
    "                          two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                          this_steps=steps[i],\n",
    "                          this_offsets=offsets[i],\n",
    "                          clip_boxes=clip_boxes,\n",
    "                          variances=variances,\n",
    "                          coords=coords,\n",
    "                          normalize_coords=normalize_coords,\n",
    "                          name='anchors6')\n",
    "        layer_anchors_wh = anchors.call(x)\n",
    "        layer_anchors_wh = np.reshape(layer_anchors_wh[0], (-1,8))[:,:4]\n",
    "        anchor_boxes_wh.append(layer_anchors_wh)\n",
    "    anchor_boxes_wh = np.vstack([boxes for boxes in anchor_boxes_wh])\n",
    "    return anchor_boxes_wh\n",
    "\n",
    "def get_gt_boxes(xml_path):\n",
    "    gt_boxes = []\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for member in root.findall('object'):\n",
    "        bndbox = member.find('bndbox')\n",
    "        box = [int(bndbox.find('xmin').text),\n",
    "               int(bndbox.find('ymin').text),\n",
    "               int(bndbox.find('xmax').text),\n",
    "               int(bndbox.find('ymax').text),]\n",
    "        gt_boxes.append(box)\n",
    "    return gt_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288178d",
   "metadata": {},
   "source": [
    "# TEST ANCHORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db98b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anchor_recall_from_xml(images_folder, scales, aspect_ratios=[0.5,1.0,2.0], aspect_ratios_per_layer=None, iou_threshold=0.3):\n",
    "    all_ious = np.array([])\n",
    "    if aspect_ratios_per_layer is None:\n",
    "        aspect_ratios = [aspect_ratios]*3\n",
    "    else:\n",
    "        aspect_ratios = aspect_ratios_per_layer\n",
    "    anchor_boxes_wh = get_anchors(scales= scales,\n",
    "                                 aspect_ratios=aspect_ratios)\n",
    "\n",
    "    anchor_boxes_wh[:,[0,2]] *= img_width\n",
    "    anchor_boxes_wh[:,[1,3]] *= img_height\n",
    "    for xml_path in glob.glob(f'{images_folder}/*.xml')[::20]:\n",
    "        img_path = xml_path.split('.')[0] + '.jpg'\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "\n",
    "        gt_boxes = np.array(get_gt_boxes(xml_path))\n",
    "        gt_boxes[:,[0,2]] = gt_boxes[:,[0,2]]/img.shape[1]*img_width\n",
    "        gt_boxes[:,[1,3]] = gt_boxes[:,[1,3]]/img.shape[0]*img_height\n",
    "\n",
    "        gt_boxes_wh = convert_xy_to_wh(gt_boxes)\n",
    "\n",
    "\n",
    "        # anchors_xy = convert_wh_to_xy(anchors_wh)\n",
    "        # preds = convert_wh_to_xy(preds)\n",
    "        max_iou, positive_mask = match_iou(gt_boxes_wh.astype(float), anchor_boxes_wh.astype(float))\n",
    "        all_ious = np.append(all_ious, max_iou.numpy())\n",
    "    return np.mean(all_ious>iou_threshold)\n",
    "\n",
    "def compute_anchor_recall_from_csv(df, scales, aspect_ratios=[0.5,1.0,2.0], aspect_ratios_per_layer=None, iou_threshold=0.3):\n",
    "    all_ious = np.array([])\n",
    "    if aspect_ratios_per_layer is None:\n",
    "        aspect_ratios = [aspect_ratios]*3\n",
    "    else:\n",
    "        aspect_ratios = aspect_ratios_per_layer\n",
    "    anchor_boxes_wh = get_anchors(scales= scales,\n",
    "                                 aspect_ratios=aspect_ratios)\n",
    "\n",
    "    anchor_boxes_wh[:,[0,2]] *= img_width\n",
    "    anchor_boxes_wh[:,[1,3]] *= img_height\n",
    "\n",
    "    for _ in range(100):\n",
    "        gt_boxes = []\n",
    "        for _ in range(10):\n",
    "            i = np.random.randint(0, df.shape[0])\n",
    "            box = df.iloc[i][['xmin','ymin','xmax','ymax']]\n",
    "            gt_boxes.append(box)\n",
    "\n",
    "\n",
    "        gt_boxes = np.array(gt_boxes)\n",
    "\n",
    "        gt_boxes_wh = convert_xy_to_wh(gt_boxes)\n",
    "\n",
    "\n",
    "        max_iou, positive_mask = match_iou(gt_boxes_wh.astype(float), anchor_boxes_wh.astype(float))\n",
    "        all_ious = np.append(all_ious, max_iou.numpy())\n",
    "    return np.mean(all_ious>iou_threshold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a469cbaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (6,) into shape (4,4,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [32], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m aspect_ratios \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!img_classes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcompute_anchor_recall_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                               \u001b[49m\u001b[43maspect_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                               \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [30], line 36\u001b[0m, in \u001b[0;36mcompute_anchor_recall_from_csv\u001b[1;34m(df, scales, aspect_ratios, aspect_ratios_per_layer, iou_threshold)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     aspect_ratios \u001b[38;5;241m=\u001b[39m aspect_ratios_per_layer\n\u001b[1;32m---> 36\u001b[0m anchor_boxes_wh \u001b[38;5;241m=\u001b[39m \u001b[43mget_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                             \u001b[49m\u001b[43maspect_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect_ratios\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m anchor_boxes_wh[:,[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m]] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m img_width\n\u001b[0;32m     40\u001b[0m anchor_boxes_wh[:,[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m]] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m img_height\n",
      "Cell \u001b[1;32mIn [29], line 28\u001b[0m, in \u001b[0;36mget_anchors\u001b[1;34m(scales, aspect_ratios, n_predictor_layers, coords)\u001b[0m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m3\u001b[39m,))\n\u001b[0;32m     15\u001b[0m anchors \u001b[38;5;241m=\u001b[39m AnchorBoxes(img_height,\n\u001b[0;32m     16\u001b[0m                   img_width,\n\u001b[0;32m     17\u001b[0m                   this_scale\u001b[38;5;241m=\u001b[39mscales[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m                   normalize_coords\u001b[38;5;241m=\u001b[39mnormalize_coords,\n\u001b[0;32m     27\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manchors6\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m layer_anchors_wh \u001b[38;5;241m=\u001b[39m \u001b[43manchors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m layer_anchors_wh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(layer_anchors_wh[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m))[:,:\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m     30\u001b[0m anchor_boxes_wh\u001b[38;5;241m.\u001b[39mappend(layer_anchors_wh)\n",
      "File \u001b[1;32mD:\\work\\test_comp_vision\\test_for_MindSet\\keras_layers\\keras_layer_AnchorBoxes.py:214\u001b[0m, in \u001b[0;36mAnchorBoxes.call\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    212\u001b[0m boxes_tensor[:, :, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(cx_grid, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_boxes)) \u001b[38;5;66;03m# Set cx\u001b[39;00m\n\u001b[0;32m    213\u001b[0m boxes_tensor[:, :, :, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(cy_grid, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_boxes)) \u001b[38;5;66;03m# Set cy\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[43mboxes_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m wh_list[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# Set w\u001b[39;00m\n\u001b[0;32m    215\u001b[0m boxes_tensor[:, :, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m wh_list[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Set h\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# Convert `(cx, cy, w, h)` to `(xmin, xmax, ymin, ymax)`\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (6,) into shape (4,4,4)"
     ]
    }
   ],
   "source": [
    "scales = [1.0, 1.0, 1.0, 1.0]\n",
    "aspect_ratios = [ 1.0, 1.0, 1.0]\n",
    "df = pd.read_csv(\"!img_classes.csv\")\n",
    "\n",
    "compute_anchor_recall_from_csv(df,\n",
    "                               scales,\n",
    "                               aspect_ratios,\n",
    "                               iou_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ac7627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>212</td>\n",
       "      <td>236</td>\n",
       "      <td>116</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>426</td>\n",
       "      <td>512</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>213</td>\n",
       "      <td>237</td>\n",
       "      <td>116</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>426</td>\n",
       "      <td>512</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>213</td>\n",
       "      <td>236</td>\n",
       "      <td>116</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12417</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>424</td>\n",
       "      <td>512</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12418</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>293</td>\n",
       "      <td>314</td>\n",
       "      <td>115</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12419</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>281</td>\n",
       "      <td>290</td>\n",
       "      <td>67</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12420</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>461</td>\n",
       "      <td>491</td>\n",
       "      <td>230</td>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12421</th>\n",
       "      <td>D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...</td>\n",
       "      <td>424</td>\n",
       "      <td>512</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12422 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_name  xmin  xmax  ymin  \\\n",
       "0      D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   212   236   116   \n",
       "1      D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   426   512    11   \n",
       "2      D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   213   237   116   \n",
       "3      D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   426   512    11   \n",
       "4      D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   213   236   116   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "12417  D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   424   512    12   \n",
       "12418  D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   293   314   115   \n",
       "12419  D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   281   290    67   \n",
       "12420  D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   461   491   230   \n",
       "12421  D:/Program Files/ML-NN/traind_ssd/DATASET\\ZAVO...   424   512    12   \n",
       "\n",
       "       ymax  class_id  \n",
       "0       141         1  \n",
       "1        61         2  \n",
       "2       141         1  \n",
       "3        61         2  \n",
       "4       141         1  \n",
       "...     ...       ...  \n",
       "12417    68         2  \n",
       "12418   164         1  \n",
       "12419    91         1  \n",
       "12420   272         1  \n",
       "12421    68         2  \n",
       "\n",
       "[12422 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c75c88f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8656821378340366"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_anchor_recall_from_xml(scales=[0.025, 0.05, 0.1, 0.2],\n",
    "                      aspect_ratios=[ 0.5, 1.0, 2.0,],\n",
    "                      images_folder='W:Work/data/DATASETS/CAR_PERSON/Stroika_ulitca/1/Images_xml',\n",
    "                      iou_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1d7e289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(36, 64), (18, 32), (9, 16)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeb850ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005, 1e-05, 1e-05, 0.0001] 0.0008051529790660225\n",
      "[0.01, 1e-05, 1e-05, 0.0001] 0.008856682769726247\n",
      "[0.015, 1e-05, 1e-05, 0.0001] 0.037037037037037035\n",
      "[0.02, 1e-05, 1e-05, 0.0001] 0.12077294685990338\n",
      "[0.025, 1e-05, 1e-05, 0.0001] 0.29307568438003223\n",
      "[0.03, 1e-05, 1e-05, 0.0001] 0.46980676328502413\n",
      "[0.035, 1e-05, 1e-05, 0.0001] 0.532608695652174\n",
      "[0.04, 1e-05, 1e-05, 0.0001] 0.5615942028985508\n",
      "[0.045, 1e-05, 1e-05, 0.0001] 0.5462962962962963\n",
      "[1e-05, 0.01, 1e-05, 0.0001] 0.009259259259259259\n",
      "[1e-05, 0.02, 1e-05, 0.0001] 0.0499194847020934\n",
      "[1e-05, 0.03, 1e-05, 0.0001] 0.06602254428341385\n",
      "[1e-05, 0.04, 1e-05, 0.0001] 0.10185185185185185\n",
      "[1e-05, 0.05, 1e-05, 0.0001] 0.11553945249597423\n",
      "[1e-05, 0.06, 1e-05, 0.0001] 0.13526570048309178\n",
      "[1e-05, 0.07, 1e-05, 0.0001] 0.1356682769726248\n",
      "[1e-05, 0.08, 1e-05, 0.0001] 0.13365539452495975\n",
      "[1e-05, 0.09, 1e-05, 0.0001] 0.12962962962962962\n",
      "[1e-05, 1e-05, 0.015, 0.0001] 0.004025764895330112\n",
      "[1e-05, 1e-05, 0.03, 0.0001] 0.016908212560386472\n",
      "[1e-05, 1e-05, 0.045, 0.0001] 0.016505636070853463\n",
      "[1e-05, 1e-05, 0.06, 0.0001] 0.016908212560386472\n",
      "[1e-05, 1e-05, 0.07500000000000001, 0.0001] 0.016908212560386472\n",
      "[1e-05, 1e-05, 0.09, 0.0001] 0.036231884057971016\n",
      "[1e-05, 1e-05, 0.10500000000000001, 0.0001] 0.0535426731078905\n",
      "[1e-05, 1e-05, 0.12, 0.0001] 0.0535426731078905\n",
      "[1e-05, 1e-05, 0.135, 0.0001] 0.0535426731078905\n",
      "[1e-05, 1e-05, 1e-05, 0.02] 0.0\n",
      "[1e-05, 1e-05, 1e-05, 0.04] 0.0\n",
      "[1e-05, 1e-05, 1e-05, 0.06] 0.0\n",
      "[1e-05, 1e-05, 1e-05, 0.08] 0.0\n",
      "[1e-05, 1e-05, 1e-05, 0.1] 0.0\n",
      "[1e-05, 1e-05, 1e-05, 0.12] 0.0\n",
      "[1e-05, 1e-05, 1e-05, 0.14] 0.0\n",
      "[1e-05, 1e-05, 1e-05, 0.16] 0.0\n",
      "[1e-05, 1e-05, 1e-05, 0.18] 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04, 0.07, 0.10500000000000001, 0.0001]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_scales = [0.00001, 0.00001, 0.00001,0.0001]\n",
    "best_scales = cur_scales\n",
    "for ind in range(4):\n",
    "    best_recall = 0\n",
    "    cur_scales = [0.00001, 0.00001, 0.00001,0.0001]\n",
    "    for n in range(1,10):\n",
    "        cur_scales[ind] = 0.005*n*(1+ind)\n",
    "        recall = compute_anchor_recall_from_xml(scales=cur_scales,\n",
    "                      aspect_ratios=[0.3,0.8,1.5],\n",
    "                      images_folder='E:/Desktop/Work/data/PERSON_CAR/Stroika_ulitca/1/Images_xml')\n",
    "        if recall>best_recall:\n",
    "            best_scales[ind] = cur_scales[ind]\n",
    "            best_recall = recall\n",
    "        print(cur_scales, recall)\n",
    "best_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f1a419e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807897545357525"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_anchor_recall(scales=[0.02, 0.06, 0.1, 0.2],\n",
    "                      aspect_ratios=[0.5,1.0,2.0],\n",
    "                      images_folder='E:/Downloads/stroika_ulitca/1/Images_xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6bb4b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "45fc85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((1,128,128,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "245bdfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 128, 128, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([img]*3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fc62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
