{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91dd470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6f1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import idx2numpy as idx\n",
    "import time\n",
    "\n",
    "# from matplotlib import pyplot as plt       # чтобы выводить промежуточные фото в jupyter\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense, Reshape, LSTM, BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import backend as K\n",
    "from keras.constraints import maxnorm\n",
    "from datetime import datetime\n",
    "\n",
    "# Список всех настроечных параметров/констант\n",
    "WORK_DIR = 'pass_photos'\n",
    "TEMP_DIR = 'pass_temp'\n",
    "#DATESET_IMG = r\"D:\\work\\test_comp_vision\\datasets\\!_lines_w25_dataset_images_100k.idx\"\n",
    "#DATASET_CLS = r\"D:\\work\\test_comp_vision\\datasets\\!_lines_w25_dataset_classes_100k.idx\"\n",
    "DATASET_IMG = r'D:\\work\\test_comp_vision\\datasets\\!_lines_w25_dataset_images_100k_upper.idx'\n",
    "DATASET_CLS = r'D:\\work\\test_comp_vision\\datasets\\!_lines_w25_dataset_classes_100k_upper.idx'\n",
    "MODEL_PATH = 'ru_emnist_letters_100k_b64_e150.h5'\n",
    "# TEST_FILE = 'pass_photos/1.jpeg'\n",
    "IMG_HEIGHT = 1000            # требуемый размер фото для нормализации всех изображений\n",
    "IMG_WIDTH = 600              # т.к. в задачу входит прочитать только ФИО, обрезаю серию/номер чтобы не усложнять распознавание\n",
    "INDENT_LEFT = 220            # обрезаем фото т.к. без него получается лучше разделить фото на куски текста\n",
    "INDENT_TOP = 40              # обрезаем лишнюю часть паспорта снизу\n",
    "INDENT_BOTTOM = 120          # обрезаем нижние поля\n",
    "SCALE_FACTOR = 8             # во сколько раз увеличиваем вырезанные слова для дальнейшей обработки букв\n",
    "DATASET_SYMBOL_SIZE = 28     # размер изображений в тренировочном датасете      \n",
    "# LABELS = '0123456789АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "LABELS = '12456789АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'   # for test 2\n",
    "SYMBOLS_COUNT = len(LABELS)  # количество символов в датасете: 33 + 33 + 10 (заглавные, строчные, цифры)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb30b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения списка файлов из каталога с фотографиями (как в task_1 и task_2)\n",
    "# TODO: переделать функцию, чтобы принимала в кач-ве параметра regex с перечислением искомых расширений файла\n",
    "def get_files(directory: str) -> list:\n",
    "    names = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            names.append(os.path.join(directory, filename))\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184a289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Масштабирование изображения\n",
    "def scale_image(image, scale):     # принимаем объект изображения OpenCV\n",
    "    \n",
    "    # получаем текущий размер, вычисляем искомый и создаем измененное изображение\n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    img_width = int(width * scale)\n",
    "    img_height = int(height * scale)\n",
    "    img = cv2.resize(image, (img_width, img_height))\n",
    "    #img = cv2.resize(image, (img_width, img_height), interpolation=cv2.INTER_CUBIC) # рекомендуют, но качество страдает\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756c6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - это не пригодилось. Зря переусложнено. Но возможно без него я и получаю ошибку при распознавании\n",
    "def normalize_img_size(image, size):\n",
    "        h, w = image.shape[0], image.shape[1]     # сначала передается высота, потом ширина\n",
    "        size_max = max(w, h)\n",
    "        letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8)\n",
    "        if w > h:\n",
    "            y_pos = size_max//2 - h//2\n",
    "            letter_square[y_pos:y_pos + h, 0:w] = letter_crop\n",
    "        elif w < h:\n",
    "            x_pos = size_max//2 - w//2\n",
    "            letter_square[0:h, x_pos:x_pos + w] = letter_crop\n",
    "        else:\n",
    "            letter_square = letter_crop\n",
    "\n",
    "        # Resize letter to 28x28 and add letter and its X-coordinate\n",
    "        letters.append((x, w, cv2.resize(letter_square, (out_size, out_size), interpolation=cv2.INTER_AREA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40284c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация размеров фотографии паспорта и вырезка нужной части для обработки\n",
    "def cut_passport_info_area(image):     # принимаем объект изображения OpenCV\n",
    "    \n",
    "    # нормализуем фото к нужному размеру\n",
    "    old_height = image.shape[0]     # получаем исходную высоту\n",
    "    resize_scale = IMG_HEIGHT / old_height       # считаем коэффициент масштабирования изображения до требуемого\n",
    "    img = scale_image(image=image, scale=resize_scale)\n",
    "    new_width = img.shape[1]      # получаем новую ширину\n",
    "    \n",
    "    # обрезаем паспорт до страницы с фото\n",
    "    x0 = INDENT_LEFT                            # отступ слева, т.к. корочка и фото нам не важны\n",
    "    y0 = IMG_HEIGHT // 2 + INDENT_TOP           # обрезка сверху, т.к. верхняя страница с местом выдачи нам не важна \n",
    "    x1 = new_width if new_width < IMG_WIDTH else IMG_WIDTH   # обрезаем все лишнее справа, если есть разворот с пропиской\n",
    "    y1 = IMG_HEIGHT - INDENT_BOTTOM\n",
    "    img = img[y0:y1, x0:x1]              # сохраняем вырезанный кусок изображения для передачи\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c504ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка изображений для распознавания текста\n",
    "def normalize_color(image):         # принимаем объект изображения OpenCV\n",
    "    \n",
    "    # обесцвечиваем, если картинка цветная\n",
    "    if len(image.shape) > 2:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)    # преобразуем в ЧБ\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # Размытие для снижения количества шумов. Эксперимент показал, что без него буквы детектируются лучше\n",
    "    # blur = cv2.GaussianBlur(gray, (5,5), 0)         # коэффициент размытия подобран вручную\n",
    "    \n",
    "    # Очередность преобраозвания найдена опытным путем\n",
    "    # TODO - при распознавании преобразуем в uint32, возможно и здесь стоит\n",
    "    kernel = np.ones((5,5), 'uint8')    \n",
    "    # kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))  # не знаю в чем разница, но так работает хуже\n",
    "    \n",
    "    # В теории erode - делает буквы тоньше, а dilate - толще: https://docs.opencv.org/3.4/db/df6/tutorial_erosion_dilatation.html\n",
    "    img_block = cv2.erode(gray, kernel, iterations=1)   # Но на практике \"жирность\" букв при этой операции повышается\n",
    "    #img_block = cv2.dilate(img_block, kernel, iterations=1)  # А тут - наоборот\n",
    "    \n",
    "    # TODO - поиграться с настройками, чтобы выдавать на выход именно контраст. Сейчас это только для детекции границ букв\n",
    "    _, img_block = cv2.threshold(img_block, 0, 255, cv2.THRESH_OTSU, cv2.THRESH_BINARY_INV) # Повышаем контраст\n",
    "    img_block = cv2.morphologyEx(img_block, cv2.MORPH_OPEN, kernel, iterations=1) # Снижаем шум на фоне\n",
    "    # img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Попытка найти лучший вариант детекции и выходного изображения. Оставил для дальнейших тестов\n",
    "    # Grayscale, Gaussian blur, Otsu's threshold\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Morph open to remove noise and invert image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    closing = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    erosion = cv2.erode(gray, kernel, iterations = 1)\n",
    "    dilation = cv2.dilate(gray, kernel, iterations = 1)\n",
    "    invert = 255 - closing\n",
    "    \n",
    "    # Повышение контраста\n",
    "    if len(image.shape) > 2:\n",
    "        imghsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        imghsv[:,:,2] = [[max(pixel - 25, 0) if pixel < 190 else min(pixel + 25, 255) for pixel in row] for row in imghsv[:,:,2]]\n",
    "        contrast = cv2.cvtColor(imghsv, cv2.COLOR_HSV2BGR)\n",
    "        gray_contrast = cv2.cvtColor(contrast, cv2.COLOR_BGR2GRAY)    # преобразуем в ЧБ\n",
    "        \n",
    "    # при коэффициенте 3 - лучше распознается Васлевский, при 5 - Соколов и Юмакаева\n",
    "    img_symbol = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 3, 2)\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_TOZERO+cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return img_block, gray      # Возвращаем контрастную картинку с разбивкой на блоки и простое ЧБ изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6fd772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем элементы текста из изображения\n",
    "def search_blocks(image, limit: int, sort_by: str, sort_reverse=False):\n",
    "    #::limit:: - необходим чтобы указать на сколько мелкие символы нам не нужно распознавать\n",
    "    \n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    # получаем контуры больших пятен на изображении, внутри которых спрятан текст\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    # contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) # другая вариация\n",
    "    \n",
    "    # print(f'Count of Block counoturs: {len(contours)}')\n",
    "    blocks = []\n",
    "    for idx, contour in enumerate(contours):\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        # print(\"R\", x, y, w, h, hierarchy[0][idx])\n",
    "        # hierarchy[i][0]: следующий контур текущего уровня\n",
    "        # hierarchy[i][1]: предыдущий контур текущего уровня\n",
    "        # hierarchy[i][2]: первый вложенный элемент\n",
    "        # hierarchy[i][3]: родительский элемент\n",
    "        # if hierarchy[0][idx][3] == 0:               # если элемент не является самым крупным\n",
    "        # cv2.rectangle(image, (x, y), (x + w, y + h), (70, 0, 0), 1) # для контрольной картинки\n",
    "        \n",
    "        if limit < h < height and limit < w < width:    # игнорируем маленькие блоки, а также блок размером с изображение\n",
    "            block = image[y:y + h, x:x + w]     # вырезаем найденный блок из изображения\n",
    "            \n",
    "            # сохраняем габариты и изображение блока в список блоков. Загоняем в словарь, чтобы проще сортировать\n",
    "            # todo: По 'x' мы определяем очередность букв, ведь чем \"левее буква\", тем меньше ее 'x'. Также можно по 'y'\n",
    "            blocks.append({'idx': idx, 'y': y, 'h': h, 'x': x, 'w': w, 'block': block})\n",
    "    \n",
    "    # Сортируем по нужному ключу: 'y' для вертикали или 'x' по горизонтали. Так же можно и по индексу или размерам\n",
    "    blocks.sort(key=lambda x: x.get(sort_by), reverse=sort_reverse)\n",
    "    # print(blocks)\n",
    "    return blocks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3051b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Если детектор букв выдает нам слишком \"широкий\" блок - значит он склеил несколько соседних букв.\n",
    "Это возможно по двум причинам:\n",
    "1. Плохое качество печати/изображения, тогда действительно соседние буквы сливаются даже для человеческого взгляда.\n",
    "2. Плохое качество фильтра определения границ букв. С этим еще нужно поработать - поковырять параметры в normalize_color()\n",
    "Выход - решать проблему математически (на вскидку). Если ширина больше высоты на определенную константу (подобрана руками)\n",
    "то делим изображение на расчетное количество элементов.\n",
    "Это не панацея, т.к. в зависимости от шрифта буква \"Ж\" может быть шире, чем сочетание \"СТ\". С английскими \"ij\" еще хуже.\n",
    "\"\"\"\n",
    "def cut_blocks(image):\n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    C = 1.2       # просто коэффициент, рассчитанный на широкие буквы вроде Ж, М, Ш и т.д., чтобы их не резало\n",
    "    if width < height*C:\n",
    "        # print(f'One symbol is True')\n",
    "        return [image]\n",
    "    else:\n",
    "        #print(f'One symbol is FALSE')\n",
    "        result = []\n",
    "        y, h, = 0, height      # высота и верхняя точке среза - всегда неизменны\n",
    "        symbol_count = math.ceil(width / height)    # округляем символы до большего целого\n",
    "        symbol_width = math.floor(width / symbol_count)   # округляем ширину в пикселях до меньшего целого\n",
    "        \n",
    "        for i in range(symbol_count):\n",
    "            x = i * symbol_width\n",
    "            result.append(image[y:h, x:x+symbol_width])\n",
    "            # print(f'y = {y}, h = {h}, x = {x}, symbol_width = {x+symbol_width}, width = {width}')\n",
    "            # print(f'symbol {i} is:\\n{result[i]}')\n",
    "            \n",
    "        # print(f'Count of separeted symbols: {len(result)}')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e15fc",
   "metadata": {},
   "source": [
    "### Детекция данных из паспорта и сохранение фоток букв в файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c683ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pass_photos\\\\0.jpeg', 'pass_photos\\\\1.jpeg', 'pass_photos\\\\2.jpeg']\n"
     ]
    }
   ],
   "source": [
    "passports = get_files(WORK_DIR)\n",
    "print(passports[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23503b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def passport_data_parser(work_dir: str, count: int):\n",
    "    # Запускаем цикл по всем фото в рабочей папке\n",
    "    try:     # TODO добавить проверку на существование файла\n",
    "        passports = get_files(work_dir)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "        \n",
    "    export_words = []\n",
    "    count = min(count, len(passports))     # Что меньше - по такой индекс и забираем фотки (для тестов)\n",
    "    \n",
    "    for id_p, passport in enumerate(passports[:count]):     # идем по списку путей к изображениям (ограничив длину списка)\n",
    "        temp_dir = os.path.join(TEMP_DIR, str(id_p))\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.mkdir(temp_dir)                              # создаем папку для сохранения промежуточных картинок\n",
    "\n",
    "        print(f'==== Image {id_p}.jpg =====')\n",
    "        image = cut_passport_info_area(cv2.imread(passport))     # получаем кусок паспорта с ФИО\n",
    "        img_blocks, img_gray = normalize_color(image=image)      # img_gray используем для передачи дальше\n",
    "        \n",
    "        # TODO - убрать сохранение промежутоных файлов, они используются только для визуального контроля\n",
    "        cv2.imwrite(f'{TEMP_DIR}/{id_p}_blocs.jpg', img_blocks)\n",
    "        cv2.imwrite(f'{TEMP_DIR}/{id_p}_symbols.jpg', image)\n",
    "\n",
    "        words = search_blocks(image=img_blocks, limit=15, sort_by='y')   # ищем блоки на картинке с жирными буквами\n",
    "        # cv2.imshow('The First Word', words[0]['block'])\n",
    "        # cv2.waitKey(0)\n",
    "        # print(f'Count of words: {words}')\n",
    "\n",
    "        # получаем все обнаруженные слова из файла, в котором читаются символы\n",
    "        for id_w, word in enumerate(words[:3]):    # можно забирать только первые 3 слова ФИО\n",
    "            export_words.append([None])      # добавляем вложенный список для каждого слова\n",
    "            # из словаря обнаруженного блока текста забираем координаты и размер блока\n",
    "            y, h, x, w = word['y'], word['h'], word['x'], word['w']\n",
    "            img_word = img_gray[y:y + h, x:x + w]     # вырезаем слово из серой картинки по его координатам\n",
    "            # img_word = image[y:y + h, x:x + w]     # вариант с повышением контраста, поэкспериментировать\n",
    "            \n",
    "            img_word = scale_image(img_word, SCALE_FACTOR)   # увеличиваем изображение, чтобы детектировать буквы\n",
    "            cv2.imwrite(os.path.join(temp_dir, f'{id_w}.jpg'), img_word)   #сохраняем файлы только для контроля\n",
    "\n",
    "            word_blocks, word_text = normalize_color(image=img_word) # прогоняем через детектор увеличенное фото слова\n",
    "            #word_blocks, word_text = normalize_color(image=word_text)    # вариант с повышением контраста\n",
    "            symbols = search_blocks(image=word_blocks, limit=SCALE_FACTOR*10, sort_by='x')\n",
    "            # print(f'Count of symbols: {len(symbols)}')\n",
    "            \n",
    "            # TODO - частичный повтор кода. Придумать как переделать, чтобы не дублировать функционал\n",
    "            for id_s, symbol in enumerate(symbols):\n",
    "                # TODO - убрать сохранение промежутоных файлов, они используются только для визуального контроля\n",
    "                word_dir = os.path.join(temp_dir, str(id_w))     # создаем очередную вложенную папку для котроля\n",
    "                if not os.path.exists(word_dir):\n",
    "                    os.mkdir(word_dir)\n",
    "\n",
    "                y, h, x, w = symbol['y'], symbol['h'], symbol['x'], symbol['w']\n",
    "                img_symbol = word_text[y:y + h, x:x + w]\n",
    "                #cv2.imwrite(os.path.join(word_dir, f'{symbol[0]}-{e}.jpg'), img_symbol)\n",
    "\n",
    "                # Доп. проверка на случай, если буквы плохо отделились\n",
    "                for id_o, one_symbol in enumerate(cut_blocks(img_symbol)):\n",
    "                    # one_symbol = cv2.resize(one_symbol, (DATASET_SYMBOL_SIZE, DATASET_SYMBOL_SIZE), interpolation=cv2.INTER_AREA)\n",
    "                    one_symbol = cv2.resize(one_symbol, (DATASET_SYMBOL_SIZE, DATASET_SYMBOL_SIZE))\n",
    "                    cv2.imwrite(os.path.join(word_dir, f'{id_s}-{id_o}.jpg'), one_symbol)\n",
    "                    export_words[id_w].append(one_symbol)\n",
    "                    \n",
    "    return export_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c574a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Image 0.jpg =====\n",
      "==== Image 1.jpg =====\n",
      "==== Image 2.jpg =====\n",
      "==== Image 3.jpg =====\n",
      "==== Image 4.jpg =====\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "symbols = []\n",
    "symbols.append(passport_data_parser(work_dir=WORK_DIR, count=5))   # Для теста разбираем только 1 паспорт\n",
    "print(len(symbols[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24aaac0",
   "metadata": {},
   "source": [
    "### Готовим модель и train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df8c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгатавливаем модель для распознавания букв из датасетов по аналогии с EMNIST\n",
    "def main_model(img_size, lb_count):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='valid',\n",
    "                            input_shape=(img_size, img_size, 1), activation='relu'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(lb_count, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "820e8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_int(labels=LABELS) -> dict:\n",
    "    label_nums = {}\n",
    "    for i, lab in enumerate(labels):\n",
    "        label_nums[lab] = i\n",
    "    print(label_nums)\n",
    "    return label_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae630dd1",
   "metadata": {},
   "source": [
    "## Загружаем датасет и разбиваем на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10173127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 1, '4': 2, '5': 3, '6': 4, '7': 5, '8': 6, '9': 7, 'А': 8, 'Б': 9, 'В': 10, 'Г': 11, 'Д': 12, 'Е': 13, 'Ё': 14, 'Ж': 15, 'З': 16, 'И': 17, 'Й': 18, 'К': 19, 'Л': 20, 'М': 21, 'Н': 22, 'О': 23, 'П': 24, 'Р': 25, 'С': 26, 'Т': 27, 'У': 28, 'Ф': 29, 'Х': 30, 'Ц': 31, 'Ч': 32, 'Ш': 33, 'Щ': 34, 'Ъ': 35, 'Ы': 36, 'Ь': 37, 'Э': 38, 'Ю': 39, 'Я': 40}\n",
      "X_train: (80000, 28, 28)\n",
      "X_test: (20000, 28, 28)\n",
      "y_train: (80000,)\n",
      "y_test: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# Загружаем датасет Часть 1 / 2\n",
    "labels_comparison = labels_to_int()     # получаем сопоставление символа к коду из датасета\n",
    "labels_symbol = list(labels_comparison.keys())     # и получаем отдельно список по символам и кодам\n",
    "labels_class = list(labels_comparison.values())\n",
    "\n",
    "#TODO - попробовать сгенерить модель на ru-EMNIST вместо моего датасета!!!! Возможно проблема именно в нем\n",
    "# Загружаем датасеты картинок и их классов\n",
    "ds_images = idx.convert_from_file(DATASET_IMG)\n",
    "ds_classes = idx.convert_from_file(DATASET_CLS)\n",
    "\n",
    "# Разбиваем выборки на train, test\n",
    "# TODO переписать под train, test, validate\n",
    "X_train, X_test = np.split(ds_images, [int(.8*len(ds_images))])\n",
    "y_train, y_test = np.split(ds_classes, [int(.8*len(ds_classes))])\n",
    "\n",
    "#print(labels_symbol)\n",
    "#print(labels_class)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "620d3ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 28, 28, 1) (80000,) (20000, 28, 28, 1) (20000,) 41\n",
      "(80000, 41) (20000, 41)\n"
     ]
    }
   ],
   "source": [
    "# Загружаем датасет Часть 2 / 2\n",
    "\n",
    "# зачем-то предлагают сделать решейп датасета картинок, добавляя к слою изображения еще одно измерение\n",
    "# такой же решейп предлагается для отправляемого в готовую модель изображения. хз надо ли это делать тут и там\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1))   ### 1 Убрать решейп здесь и в функции predict_img()\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Это тупо уменьшаем выборку из датасетов в 10 раз\n",
    "#k = 10\n",
    "#X_train = X_train[:X_train.shape[0] // k]\n",
    "#y_train = y_train[:y_train.shape[0] // k]\n",
    "#X_test = X_test[:X_test.shape[0] // k]\n",
    "#y_test = y_test[:y_test.shape[0] // k]\n",
    "\n",
    "# Нормализация - ХЗ что такое, но с этим преобразованием результат чуть лучше. TODO - Разобраться почему\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_train /= 255.0\n",
    "X_test = X_test.astype(np.float32)\n",
    "X_test /= 255.0\n",
    "\n",
    "# \"маска\" на которую будут созданы предсказание категорий\n",
    "#x_train_cat = keras.utils.to_categorical(y_train, len(labels_comparison))   ### !!! вероятно ошибка тут !!!!\n",
    "y_train_cat = keras.utils.to_categorical(y_train, len(labels_comparison))    ### Вот так распознавание работает!!!\n",
    "y_test_cat = keras.utils.to_categorical(y_test, len(labels_comparison))\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, len(labels_comparison))\n",
    "print(y_train_cat.shape, y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a74b07",
   "metadata": {},
   "source": [
    "## Обучаем модель на датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ff8226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1250/1250 [==============================] - 95s 75ms/step - loss: 40.5885 - accuracy: 0.0376 - val_loss: 4.0610 - val_accuracy: 0.0719 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "1250/1250 [==============================] - 105s 84ms/step - loss: 6.0120 - accuracy: 0.0345 - val_loss: 4.3286 - val_accuracy: 0.0321 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "1250/1250 [==============================] - 107s 85ms/step - loss: 4.4607 - accuracy: 0.0402 - val_loss: 4.3272 - val_accuracy: 0.0505 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 4.3545 - accuracy: 0.0496\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1250/1250 [==============================] - 104s 84ms/step - loss: 4.3545 - accuracy: 0.0496 - val_loss: 4.3134 - val_accuracy: 0.0666 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "1250/1250 [==============================] - 104s 83ms/step - loss: 4.3169 - accuracy: 0.0549 - val_loss: 4.3074 - val_accuracy: 0.0756 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "1250/1250 [==============================] - 104s 83ms/step - loss: 4.2885 - accuracy: 0.0597 - val_loss: 4.2844 - val_accuracy: 0.0958 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "1250/1250 [==============================] - 104s 83ms/step - loss: 4.2643 - accuracy: 0.0648 - val_loss: 4.2623 - val_accuracy: 0.1046 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "1250/1250 [==============================] - 106s 85ms/step - loss: 4.2410 - accuracy: 0.0701 - val_loss: 4.2313 - val_accuracy: 0.1160 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 4.2043 - accuracy: 0.0767 - val_loss: 4.1830 - val_accuracy: 0.1272 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "1250/1250 [==============================] - 103s 82ms/step - loss: 4.1653 - accuracy: 0.0822 - val_loss: 4.1337 - val_accuracy: 0.1388 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 4.1271 - accuracy: 0.0922 - val_loss: 4.0746 - val_accuracy: 0.1517 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "1250/1250 [==============================] - 103s 82ms/step - loss: 4.0657 - accuracy: 0.1034 - val_loss: 3.9792 - val_accuracy: 0.1780 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "1250/1250 [==============================] - 103s 82ms/step - loss: 4.0071 - accuracy: 0.1149 - val_loss: 3.8802 - val_accuracy: 0.2175 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "1250/1250 [==============================] - 102s 82ms/step - loss: 3.9397 - accuracy: 0.1308 - val_loss: 3.7526 - val_accuracy: 0.2706 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "1250/1250 [==============================] - 102s 82ms/step - loss: 3.8613 - accuracy: 0.1479 - val_loss: 3.6223 - val_accuracy: 0.3273 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "1250/1250 [==============================] - 103s 82ms/step - loss: 3.7835 - accuracy: 0.1664 - val_loss: 3.4932 - val_accuracy: 0.3719 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "1250/1250 [==============================] - 100s 80ms/step - loss: 3.6939 - accuracy: 0.1842 - val_loss: 3.3496 - val_accuracy: 0.4106 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 3.6021 - accuracy: 0.2086 - val_loss: 3.1929 - val_accuracy: 0.4428 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 3.5036 - accuracy: 0.2301 - val_loss: 3.0348 - val_accuracy: 0.4581 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 3.3933 - accuracy: 0.2537 - val_loss: 2.8676 - val_accuracy: 0.4895 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "1250/1250 [==============================] - 104s 83ms/step - loss: 3.2720 - accuracy: 0.2837 - val_loss: 2.7097 - val_accuracy: 0.5257 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 3.1620 - accuracy: 0.3085 - val_loss: 2.5551 - val_accuracy: 0.5529 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 3.0590 - accuracy: 0.3355 - val_loss: 2.4111 - val_accuracy: 0.5814 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 2.9584 - accuracy: 0.3623 - val_loss: 2.2949 - val_accuracy: 0.5979 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 2.8447 - accuracy: 0.3875 - val_loss: 2.1846 - val_accuracy: 0.6141 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "1250/1250 [==============================] - 121s 97ms/step - loss: 2.7587 - accuracy: 0.4127 - val_loss: 2.0881 - val_accuracy: 0.6284 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "1250/1250 [==============================] - 111s 89ms/step - loss: 2.6747 - accuracy: 0.4346 - val_loss: 1.9961 - val_accuracy: 0.6532 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "1250/1250 [==============================] - 105s 84ms/step - loss: 2.5857 - accuracy: 0.4567 - val_loss: 1.9269 - val_accuracy: 0.6809 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "1250/1250 [==============================] - 103s 83ms/step - loss: 2.5138 - accuracy: 0.4784 - val_loss: 1.8644 - val_accuracy: 0.6912 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "1250/1250 [==============================] - 103s 83ms/step - loss: 2.4520 - accuracy: 0.4963 - val_loss: 1.8210 - val_accuracy: 0.6981 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "1250/1250 [==============================] - 106s 85ms/step - loss: 2.3894 - accuracy: 0.5102 - val_loss: 1.7743 - val_accuracy: 0.7064 - lr: 5.0000e-04\n",
      "Epoch 32/150\n",
      "1250/1250 [==============================] - 104s 83ms/step - loss: 2.3275 - accuracy: 0.5276 - val_loss: 1.7353 - val_accuracy: 0.7109 - lr: 5.0000e-04\n",
      "Epoch 33/150\n",
      "1250/1250 [==============================] - 105s 84ms/step - loss: 2.2789 - accuracy: 0.5438 - val_loss: 1.7008 - val_accuracy: 0.7157 - lr: 5.0000e-04\n",
      "Epoch 34/150\n",
      "1250/1250 [==============================] - 104s 83ms/step - loss: 2.2306 - accuracy: 0.5545 - val_loss: 1.6728 - val_accuracy: 0.7192 - lr: 5.0000e-04\n",
      "Epoch 35/150\n",
      "1250/1250 [==============================] - 113s 91ms/step - loss: 2.1780 - accuracy: 0.5683 - val_loss: 1.6429 - val_accuracy: 0.7231 - lr: 5.0000e-04\n",
      "Epoch 36/150\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 2.1378 - accuracy: 0.5823 - val_loss: 1.6137 - val_accuracy: 0.7287 - lr: 5.0000e-04\n",
      "Epoch 37/150\n",
      "1250/1250 [==============================] - 106s 85ms/step - loss: 2.0988 - accuracy: 0.5903 - val_loss: 1.5892 - val_accuracy: 0.7316 - lr: 5.0000e-04\n",
      "Epoch 38/150\n",
      "1250/1250 [==============================] - 102s 82ms/step - loss: 2.0563 - accuracy: 0.5994 - val_loss: 1.5656 - val_accuracy: 0.7351 - lr: 5.0000e-04\n",
      "Epoch 39/150\n",
      "1250/1250 [==============================] - 107s 86ms/step - loss: 2.0250 - accuracy: 0.6091 - val_loss: 1.5452 - val_accuracy: 0.7395 - lr: 5.0000e-04\n",
      "Epoch 40/150\n",
      "1250/1250 [==============================] - 106s 85ms/step - loss: 1.9914 - accuracy: 0.6166 - val_loss: 1.5232 - val_accuracy: 0.7416 - lr: 5.0000e-04\n",
      "Epoch 41/150\n",
      "1250/1250 [==============================] - 103s 82ms/step - loss: 1.9570 - accuracy: 0.6259 - val_loss: 1.5036 - val_accuracy: 0.7460 - lr: 5.0000e-04\n",
      "Epoch 42/150\n",
      "1250/1250 [==============================] - 106s 85ms/step - loss: 1.9288 - accuracy: 0.6360 - val_loss: 1.4873 - val_accuracy: 0.7509 - lr: 5.0000e-04\n",
      "Epoch 43/150\n",
      "1250/1250 [==============================] - 103s 82ms/step - loss: 1.8929 - accuracy: 0.6427 - val_loss: 1.4683 - val_accuracy: 0.7521 - lr: 5.0000e-04\n",
      "Epoch 44/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 1.8744 - accuracy: 0.6483 - val_loss: 1.4536 - val_accuracy: 0.7552 - lr: 5.0000e-04\n",
      "Epoch 45/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 1.8496 - accuracy: 0.6541 - val_loss: 1.4383 - val_accuracy: 0.7584 - lr: 5.0000e-04\n",
      "Epoch 46/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 1.8223 - accuracy: 0.6608 - val_loss: 1.4250 - val_accuracy: 0.7608 - lr: 5.0000e-04\n",
      "Epoch 47/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 1.8019 - accuracy: 0.6663 - val_loss: 1.4125 - val_accuracy: 0.7638 - lr: 5.0000e-04\n",
      "Epoch 48/150\n",
      "1250/1250 [==============================] - 106s 85ms/step - loss: 1.7782 - accuracy: 0.6724 - val_loss: 1.4003 - val_accuracy: 0.7649 - lr: 5.0000e-04\n",
      "Epoch 49/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 110s 88ms/step - loss: 1.7613 - accuracy: 0.6753 - val_loss: 1.3909 - val_accuracy: 0.7668 - lr: 5.0000e-04\n",
      "Epoch 50/150\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 1.7361 - accuracy: 0.6832 - val_loss: 1.3792 - val_accuracy: 0.7685 - lr: 5.0000e-04\n",
      "Epoch 51/150\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 1.7206 - accuracy: 0.6864 - val_loss: 1.3692 - val_accuracy: 0.7697 - lr: 5.0000e-04\n",
      "Epoch 52/150\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 1.6947 - accuracy: 0.6929 - val_loss: 1.3588 - val_accuracy: 0.7716 - lr: 5.0000e-04\n",
      "Epoch 53/150\n",
      "1250/1250 [==============================] - 134s 107ms/step - loss: 1.6904 - accuracy: 0.6940 - val_loss: 1.3504 - val_accuracy: 0.7727 - lr: 5.0000e-04\n",
      "Epoch 54/150\n",
      "1250/1250 [==============================] - 126s 101ms/step - loss: 1.6702 - accuracy: 0.6996 - val_loss: 1.3406 - val_accuracy: 0.7731 - lr: 5.0000e-04\n",
      "Epoch 55/150\n",
      "1250/1250 [==============================] - 126s 100ms/step - loss: 1.6566 - accuracy: 0.7023 - val_loss: 1.3334 - val_accuracy: 0.7740 - lr: 5.0000e-04\n",
      "Epoch 56/150\n",
      "1250/1250 [==============================] - 121s 97ms/step - loss: 1.6363 - accuracy: 0.7079 - val_loss: 1.3237 - val_accuracy: 0.7748 - lr: 5.0000e-04\n",
      "Epoch 57/150\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 1.6216 - accuracy: 0.7100 - val_loss: 1.3152 - val_accuracy: 0.7753 - lr: 5.0000e-04\n",
      "Epoch 58/150\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 1.6091 - accuracy: 0.7136 - val_loss: 1.3100 - val_accuracy: 0.7756 - lr: 5.0000e-04\n",
      "Epoch 59/150\n",
      "1250/1250 [==============================] - 114s 91ms/step - loss: 1.5877 - accuracy: 0.7156 - val_loss: 1.3019 - val_accuracy: 0.7760 - lr: 5.0000e-04\n",
      "Epoch 60/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 1.5812 - accuracy: 0.7206 - val_loss: 1.2952 - val_accuracy: 0.7770 - lr: 5.0000e-04\n",
      "Epoch 61/150\n",
      "1250/1250 [==============================] - 100s 80ms/step - loss: 1.5716 - accuracy: 0.7230 - val_loss: 1.2892 - val_accuracy: 0.7768 - lr: 5.0000e-04\n",
      "Epoch 62/150\n",
      "1250/1250 [==============================] - 102s 81ms/step - loss: 1.5615 - accuracy: 0.7252 - val_loss: 1.2840 - val_accuracy: 0.7773 - lr: 5.0000e-04\n",
      "Epoch 63/150\n",
      "1250/1250 [==============================] - 101s 81ms/step - loss: 1.5534 - accuracy: 0.7280 - val_loss: 1.2777 - val_accuracy: 0.7777 - lr: 5.0000e-04\n",
      "Epoch 64/150\n",
      "1250/1250 [==============================] - 108s 86ms/step - loss: 1.5336 - accuracy: 0.7294 - val_loss: 1.2695 - val_accuracy: 0.7783 - lr: 5.0000e-04\n",
      "Epoch 65/150\n",
      "1250/1250 [==============================] - 106s 85ms/step - loss: 1.5291 - accuracy: 0.7313 - val_loss: 1.2661 - val_accuracy: 0.7789 - lr: 5.0000e-04\n",
      "Epoch 66/150\n",
      "1250/1250 [==============================] - 106s 85ms/step - loss: 1.5154 - accuracy: 0.7350 - val_loss: 1.2604 - val_accuracy: 0.7806 - lr: 5.0000e-04\n",
      "Epoch 67/150\n",
      "1250/1250 [==============================] - 103s 83ms/step - loss: 1.5058 - accuracy: 0.7380 - val_loss: 1.2550 - val_accuracy: 0.7818 - lr: 5.0000e-04\n",
      "Epoch 68/150\n",
      "1250/1250 [==============================] - 107s 85ms/step - loss: 1.4952 - accuracy: 0.7401 - val_loss: 1.2504 - val_accuracy: 0.7830 - lr: 5.0000e-04\n",
      "Epoch 69/150\n",
      "1250/1250 [==============================] - 108s 87ms/step - loss: 1.4765 - accuracy: 0.7434 - val_loss: 1.2455 - val_accuracy: 0.7833 - lr: 5.0000e-04\n",
      "Epoch 70/150\n",
      "1250/1250 [==============================] - 113s 90ms/step - loss: 1.4803 - accuracy: 0.7442 - val_loss: 1.2415 - val_accuracy: 0.7843 - lr: 5.0000e-04\n",
      "Epoch 71/150\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 1.4712 - accuracy: 0.7464 - val_loss: 1.2376 - val_accuracy: 0.7848 - lr: 5.0000e-04\n",
      "Epoch 72/150\n",
      "1250/1250 [==============================] - 117s 94ms/step - loss: 1.4636 - accuracy: 0.7474 - val_loss: 1.2327 - val_accuracy: 0.7853 - lr: 5.0000e-04\n",
      "Epoch 73/150\n",
      "1250/1250 [==============================] - 112s 90ms/step - loss: 1.4566 - accuracy: 0.7478 - val_loss: 1.2289 - val_accuracy: 0.7857 - lr: 5.0000e-04\n",
      "Epoch 74/150\n",
      "1250/1250 [==============================] - 111s 89ms/step - loss: 1.4491 - accuracy: 0.7505 - val_loss: 1.2243 - val_accuracy: 0.7860 - lr: 5.0000e-04\n",
      "Epoch 75/150\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 1.4439 - accuracy: 0.7532 - val_loss: 1.2203 - val_accuracy: 0.7868 - lr: 5.0000e-04\n",
      "Epoch 76/150\n",
      "1250/1250 [==============================] - 112s 89ms/step - loss: 1.4248 - accuracy: 0.7540 - val_loss: 1.2183 - val_accuracy: 0.7872 - lr: 5.0000e-04\n",
      "Epoch 77/150\n",
      "1250/1250 [==============================] - 112s 90ms/step - loss: 1.4298 - accuracy: 0.7551 - val_loss: 1.2153 - val_accuracy: 0.7872 - lr: 5.0000e-04\n",
      "Epoch 78/150\n",
      "1250/1250 [==============================] - 114s 91ms/step - loss: 1.4208 - accuracy: 0.7565 - val_loss: 1.2119 - val_accuracy: 0.7875 - lr: 5.0000e-04\n",
      "Epoch 79/150\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 1.4105 - accuracy: 0.7578 - val_loss: 1.2085 - val_accuracy: 0.7876 - lr: 5.0000e-04\n",
      "Epoch 80/150\n",
      "1250/1250 [==============================] - 114s 91ms/step - loss: 1.4032 - accuracy: 0.7599 - val_loss: 1.2052 - val_accuracy: 0.7883 - lr: 5.0000e-04\n",
      "Epoch 81/150\n",
      "1250/1250 [==============================] - 113s 90ms/step - loss: 1.3922 - accuracy: 0.7625 - val_loss: 1.2022 - val_accuracy: 0.7880 - lr: 5.0000e-04\n",
      "Epoch 82/150\n",
      "1250/1250 [==============================] - 117s 94ms/step - loss: 1.3907 - accuracy: 0.7634 - val_loss: 1.1992 - val_accuracy: 0.7889 - lr: 5.0000e-04\n",
      "Epoch 83/150\n",
      "1250/1250 [==============================] - 112s 90ms/step - loss: 1.3946 - accuracy: 0.7646 - val_loss: 1.1963 - val_accuracy: 0.7886 - lr: 5.0000e-04\n",
      "Epoch 84/150\n",
      "1250/1250 [==============================] - 112s 89ms/step - loss: 1.3804 - accuracy: 0.7641 - val_loss: 1.1924 - val_accuracy: 0.7888 - lr: 5.0000e-04\n",
      "Epoch 85/150\n",
      "1250/1250 [==============================] - 112s 90ms/step - loss: 1.3751 - accuracy: 0.7654 - val_loss: 1.1889 - val_accuracy: 0.7890 - lr: 5.0000e-04\n",
      "Epoch 86/150\n",
      "1250/1250 [==============================] - 112s 89ms/step - loss: 1.3713 - accuracy: 0.7676 - val_loss: 1.1867 - val_accuracy: 0.7894 - lr: 5.0000e-04\n",
      "Epoch 87/150\n",
      "1250/1250 [==============================] - 111s 88ms/step - loss: 1.3648 - accuracy: 0.7683 - val_loss: 1.1850 - val_accuracy: 0.7894 - lr: 5.0000e-04\n",
      "Epoch 88/150\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 1.3627 - accuracy: 0.7689 - val_loss: 1.1814 - val_accuracy: 0.7896 - lr: 5.0000e-04\n",
      "Epoch 89/150\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 1.3500 - accuracy: 0.7707 - val_loss: 1.1786 - val_accuracy: 0.7898 - lr: 5.0000e-04\n",
      "Epoch 90/150\n",
      "1250/1250 [==============================] - 114s 91ms/step - loss: 1.3506 - accuracy: 0.7711 - val_loss: 1.1760 - val_accuracy: 0.7898 - lr: 5.0000e-04\n",
      "Epoch 91/150\n",
      "1250/1250 [==============================] - 114s 91ms/step - loss: 1.3411 - accuracy: 0.7735 - val_loss: 1.1736 - val_accuracy: 0.7897 - lr: 5.0000e-04\n",
      "Epoch 92/150\n",
      "1250/1250 [==============================] - 113s 90ms/step - loss: 1.3401 - accuracy: 0.7730 - val_loss: 1.1722 - val_accuracy: 0.7903 - lr: 5.0000e-04\n",
      "Epoch 93/150\n",
      "1250/1250 [==============================] - 113s 91ms/step - loss: 1.3308 - accuracy: 0.7738 - val_loss: 1.1686 - val_accuracy: 0.7903 - lr: 5.0000e-04\n",
      "Epoch 94/150\n",
      "1250/1250 [==============================] - 121s 97ms/step - loss: 1.3287 - accuracy: 0.7746 - val_loss: 1.1658 - val_accuracy: 0.7907 - lr: 5.0000e-04\n",
      "Epoch 95/150\n",
      "1250/1250 [==============================] - 116s 93ms/step - loss: 1.3189 - accuracy: 0.7759 - val_loss: 1.1641 - val_accuracy: 0.7904 - lr: 5.0000e-04\n",
      "Epoch 96/150\n",
      "1250/1250 [==============================] - 112s 89ms/step - loss: 1.3214 - accuracy: 0.7762 - val_loss: 1.1623 - val_accuracy: 0.7911 - lr: 5.0000e-04\n",
      "Epoch 97/150\n",
      "1250/1250 [==============================] - 118s 94ms/step - loss: 1.3146 - accuracy: 0.7779 - val_loss: 1.1598 - val_accuracy: 0.7909 - lr: 5.0000e-04\n",
      "Epoch 98/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 116s 92ms/step - loss: 1.3133 - accuracy: 0.7771 - val_loss: 1.1572 - val_accuracy: 0.7913 - lr: 5.0000e-04\n",
      "Epoch 99/150\n",
      "1250/1250 [==============================] - 113s 90ms/step - loss: 1.3015 - accuracy: 0.7794 - val_loss: 1.1561 - val_accuracy: 0.7914 - lr: 5.0000e-04\n",
      "Epoch 100/150\n",
      "1250/1250 [==============================] - 113s 90ms/step - loss: 1.3030 - accuracy: 0.7803 - val_loss: 1.1530 - val_accuracy: 0.7919 - lr: 5.0000e-04\n",
      "Epoch 101/150\n",
      "1250/1250 [==============================] - 112s 89ms/step - loss: 1.2986 - accuracy: 0.7811 - val_loss: 1.1506 - val_accuracy: 0.7926 - lr: 5.0000e-04\n",
      "Epoch 102/150\n",
      "1250/1250 [==============================] - 112s 89ms/step - loss: 1.2876 - accuracy: 0.7813 - val_loss: 1.1494 - val_accuracy: 0.7928 - lr: 5.0000e-04\n",
      "Epoch 103/150\n",
      "1102/1250 [=========================>....] - ETA: 12s - loss: 1.2936 - accuracy: 0.7833"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# !!! проверить правильно ли заданы параметры в fit !!!\u001b[39;00m\n\u001b[0;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_cat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlearning_rate_reduction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mru_emnist_letters_100k_b\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_name)\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\Program Files\\Python3\\3.10\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "EPOCH = 150\n",
    "\n",
    "# TODO - протестировать запуск на 100+ эпох 1/10 часть датасета (10 тыс изображений)\n",
    "\n",
    "# Set a learning rate reduction\n",
    "learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "# Загружаем в модель размер изображений и количество классов\n",
    "model = main_model(DATASET_SYMBOL_SIZE, len(labels_comparison))\n",
    "\n",
    "# !!! проверить правильно ли заданы параметры в fit !!!\n",
    "start_time = datetime.now()\n",
    "model.fit(X_train, y_train_cat, validation_data=(X_test, y_test_cat), callbacks=[learning_rate_reduction], batch_size=BATCH, epochs=EPOCH)\n",
    "\n",
    "model_name = f'ru_emnist_letters_100k_b{BATCH}_e{EPOCH}_upper.h5'\n",
    "model.save(model_name)\n",
    "\n",
    "finish_time = datetime.now()\n",
    "runtime = fitish_time - start_time\n",
    "time_log = f'Started at: {start_tyme}\\nFinished at: {finish_time}\\nTotal Runtime: {runtime}'\n",
    "print(time_log)\n",
    "with open(f'{model_name}.txt', 'w') as f:\n",
    "    f.write(timelog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e890ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_saved = keras.models.load_model('ru_emnist_letters_100k_b64_e60.h5')\n",
    "\n",
    "def predict_img(model, img):\n",
    "    \n",
    "    # Эти преобразования нужны только для того, чтобы повернуть символы из датасета emnist (там буквы лежат на боку)\n",
    "    # img_arr = np.expand_dims(img, axis=0)\n",
    "    # img_arr = 1 - img_arr/255.0\n",
    "    # img_arr[0] = np.rot90(img_arr[0], 3)\n",
    "    # img_arr[0] = np.fliplr(img_arr[0])\n",
    "    # img_arr = img_arr.reshape((1, 28, 28, 1))\n",
    "    \n",
    "    img = img.reshape((1, 28, 28, 1))     ### 1 Убрать решейп здесь и в функции Часть 2 блока загрузки модели\n",
    "\n",
    "    predict = model.predict(img)\n",
    "    result = np.argmax(predict, axis=1)     # получаем индекс класса с наибольшей предсказанной вероятностью\n",
    "    \n",
    "    #TODO изменить на использование словаря соответствия класса и буквы. сейчас тупо по индексу класса забираю букву\n",
    "    # return chr(emnist_labels[result[0]])\n",
    "    print(labels_symbol[result[0]])\n",
    "    return labels_symbol[result[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd32bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# не использую. убрать!\n",
    "def img_to_str(model, word_by_imgs):\n",
    "    s_out = \"\"\n",
    "    \n",
    "    for letter in word_by_imgs:\n",
    "        s_out += predict_img(model, letter)\n",
    "    return s_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8b24ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model just created will be used.\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "в\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "е\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "е\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "р\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "а\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "з\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "н\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "а\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "п\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "о\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "м\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "с\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "т\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "ь\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "м\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "и\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "д\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "е\n",
      "(1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "А\n",
      "вееразнапомстьмидеА\n"
     ]
    }
   ],
   "source": [
    "word_images_paths = get_files(r'D:\\work\\test_comp_vision\\test_for_MindSet\\pass_temp\\0\\0_a') # тот же текст, на котором учим\n",
    "# word_images_paths = get_files(r\"D:\\work\\test_comp_vision\\test_for_MindSet\\pass_temp\\0\\1\")\n",
    "word_images = []     # сюда собираем список всех картинок для одного слова\n",
    "predicted_word = ''\n",
    "\n",
    "# Если только что занимались созданием модели - будет запущена она. Если нет - будет запущена версия с диска  \n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    print(\"The model just created will be used.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"The Model has not been created in the current session. Loading the saved model.\")\n",
    "        model = keras.models.load_model(MODEL_PATH)\n",
    "    except NameError as ne:\n",
    "        print(ne)\n",
    "    except Exception as e:     # TODO Добавить обработку отсутствия файла\n",
    "        print(e)\n",
    "\n",
    "# TODO - Переписать под использование картинок из кода выше, вместо исползования сохраненных на диск\n",
    "# Либо вынести в отдельную функцию забор картинок из папки, и в отдельную - запуск обработки полноценных изображений\n",
    "for letter_path in word_images_paths:\n",
    "    #word_images.append(cv2.imread(letter_path))\n",
    "    with Image.open(letter_path) as image:       # открываем картинку по ссылке, преобразуем в массив\n",
    "        # img_to_arr = np.asarray(image)           # преобразуем загруженную картинку к необходимой Модели форме\n",
    "        # img_to_arr = np.asarray([img_to_arr])    # требуется именно такое двойное преобразование \n",
    "        img_to_arr = np.asarray([np.asarray(image)])    # ТАК НАДО!!! Такова форма модели, иначе не работает\n",
    "        print(img_to_arr.shape)\n",
    "        #word_images.append(np.asarray(image))\n",
    "        \n",
    "        predicted_word += predict_img(model=model, img=img_to_arr)\n",
    "        \n",
    "        \n",
    "#print(word_images[0])\n",
    "\n",
    "    \n",
    "#predicted_word = img_to_str(model=model_saved, word_by_imgs=word_images)\n",
    "\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7573b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_images = idx.convert_from_file(DATESET_IMG)\n",
    "ds_images[0]\n",
    "im = Image.fromarray(np.uint8(ds_images[0]))\n",
    "#im.show()\n",
    "#im.close()\n",
    "im = Image.open(r\"D:\\work\\test_comp_vision\\test_for_MindSet\\pass_temp\\0\\0\\0-0.jpg\")\n",
    "im.show()\n",
    "im.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6382eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
