{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc8ecf0",
   "metadata": {},
   "source": [
    "### Import and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6f1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import idx2numpy as idx\n",
    "import time\n",
    "import pytesseract\n",
    "import string\n",
    "import color_normalization as cn\n",
    "\n",
    "# from matplotlib import pyplot as plt       # чтобы выводить промежуточные фото в jupyter\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense, Reshape, LSTM, BatchNormalization, RandomContrast, RandomRotation, Rescaling\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import backend as K\n",
    "from keras.constraints import maxnorm\n",
    "from keras.preprocessing.image import ImageDataGenerator # Для аугментаци (устаревший способ, TF рекомендует добавить слой препроцессинга в модель)\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# Список всех настроечных параметров/констант\n",
    "WORK_DIR = 'pass_photos'\n",
    "TEMP_DIR = 'pass_temp'\n",
    "\n",
    "DATASET_IMG = r'D:\\work\\test_comp_vision\\datasets\\!_lines_w25_dataset_images_100k_upper2.idx'\n",
    "DATASET_CLS = r'D:\\work\\test_comp_vision\\datasets\\!_lines_w25_dataset_classes_100k_upper2.idx'\n",
    "\n",
    "DS_TRAIN_IMG = r\"D:\\work\\test_comp_vision\\datasets\\!_lines_w25_parsed_fix_TRAIN_125000_UPPER_CHAR_FIXED_images.idx\"\n",
    "DS_TRAIN_LBL = r\"D:\\work\\test_comp_vision\\datasets\\!_lines_w25_parsed_fix_TRAIN_125000_UPPER_CHAR_FIXED_labels.idx\"\n",
    "DS_VALID_IMG = r\"D:\\work\\test_comp_vision\\datasets\\!_lines_w25_parsed_fix_VALID_25000_UPPER_CHAR_FIXED_images.idx\"\n",
    "DS_VALID_LBL = r\"D:\\work\\test_comp_vision\\datasets\\!_lines_w25_parsed_fix_VALID_25000_UPPER_CHAR_FIXED_labels.idx\"\n",
    "DS_TEST_IMG = r\"D:\\work\\test_comp_vision\\datasets\\!_lines_w25_parsed_fix_TEST_25000_UPPER_CHAR_FIXED_images.idx\"\n",
    "DS_TEST_LBL = r\"D:\\work\\test_comp_vision\\datasets\\!_lines_w25_parsed_fix_TEST_25000_UPPER_CHAR_FIXED_labels.idx\"\n",
    "\n",
    "MODEL_PATH = 'ru_letters_125k_b64_e150_DS-FIX_loss_0.01_acc_1.0.h5'\n",
    "# TEST_FILE = 'pass_photos/1.jpeg'\n",
    "IMG_HEIGHT = 1000            # требуемый размер фото для нормализации всех изображений\n",
    "IMG_WIDTH = 600              # т.к. в задачу входит прочитать только ФИО, обрезаю серию/номер чтобы не усложнять распознавание\n",
    "INDENT_LEFT = 220            # обрезаем фото т.к. без него получается лучше разделить фото на куски текста\n",
    "INDENT_TOP = 40              # обрезаем лишнюю часть паспорта снизу\n",
    "INDENT_BOTTOM = 120          # обрезаем нижние поля\n",
    "SCALE_FACTOR = 8             # во сколько раз увеличиваем вырезанные слова для дальнейшей обработки букв\n",
    "DATASET_SYMBOL_SIZE = 28     # размер изображений в тренировочном датасете      \n",
    "# LABELS = '0123456789АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "LABELS = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'   # for test 2\n",
    "SYMBOLS_COUNT = len(LABELS)  # количество символов в датасете: 33 + 33 + 10 (заглавные, строчные, цифры)\n",
    "SYMBOLS_MARGIN_X = 0.075       # это обозначает добавление полей вокруг текста шириной 7.5% от размера буквы слева/справа\n",
    "SYMBOLS_MARGIN_Y = 0.1       # это обозначает добавление полей вокруг текста шириной 10% от размера буквы сверху/снизу\n",
    "LANG = 'rus'                        # Язык для распознавания текста (Tesseract OCR)\n",
    "PSM = 6                             # Page Segmentation Mode (--psm). Больше информации в Tesseract --help\n",
    "OEM = 3                             # Engine Mode (--oem). Больше информации в Tesseract --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca124e",
   "metadata": {},
   "source": [
    "### Functions for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb30b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'А': 0, 'Б': 1, 'В': 2, 'Г': 3, 'Д': 4, 'Е': 5, 'Ё': 6, 'Ж': 7, 'З': 8, 'И': 9, 'Й': 10, 'К': 11, 'Л': 12, 'М': 13, 'Н': 14, 'О': 15, 'П': 16, 'Р': 17, 'С': 18, 'Т': 19, 'У': 20, 'Ф': 21, 'Х': 22, 'Ц': 23, 'Ч': 24, 'Ш': 25, 'Щ': 26, 'Ъ': 27, 'Ы': 28, 'Ь': 29, 'Э': 30, 'Ю': 31, 'Я': 32}\n"
     ]
    }
   ],
   "source": [
    "def get_files(directory: str) -> list:\n",
    "    # Функция для получения списка файлов из каталога с фотографиями (как в task_1 и task_2)\n",
    "    # TODO: переделать функцию, чтобы принимала в кач-ве параметра regex с перечислением искомых расширений файла\n",
    "    names = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            names.append(os.path.join(directory, filename))\n",
    "    return names\n",
    "\n",
    "\n",
    "def scale_image(image, scale):     # принимаем объект изображения OpenCV\n",
    "    # Масштабирование изображения\n",
    "    # получаем текущий размер, вычисляем искомый и создаем измененное изображение\n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    img_width = int(width * scale)\n",
    "    img_height = int(height * scale)\n",
    "    img = cv2.resize(image, (img_width, img_height))\n",
    "    # img = cv2.resize(image, (img_width, img_height), interpolation=cv2.INTER_CUBIC) # рекомендуют, но качество страдает\n",
    "    return img\n",
    "\n",
    "\n",
    "# TODO - это не пригодилось. Зря переусложнено. Но возможно без него я и получаю ошибку при распознавании\n",
    "def normalize_img_size(image, size):\n",
    "    h, w = image.shape[0], image.shape[1]     # сначала передается высота, потом ширина\n",
    "    size_max = max(w, h)\n",
    "    letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8)\n",
    "    if w > h:\n",
    "        y_pos = size_max//2 - h//2\n",
    "        letter_square[y_pos:y_pos + h, 0:w] = letter_crop\n",
    "    elif w < h:\n",
    "        x_pos = size_max//2 - w//2\n",
    "        letter_square[0:h, x_pos:x_pos + w] = letter_crop\n",
    "    else:\n",
    "        letter_square = letter_crop\n",
    "    # Resize letter to 28x28 and add letter and its X-coordinate\n",
    "    letters.append((x, w, cv2.resize(letter_square, (out_size, out_size), interpolation=cv2.INTER_AREA)))\n",
    "        \n",
    "\n",
    "def cut_passport_info_area(image):     # принимаем объект изображения OpenCV\n",
    "    # Нормализация размеров фотографии паспорта и вырезка нужной части для обработки\n",
    "    # нормализуем фото к нужному размеру\n",
    "    old_height = image.shape[0]     # получаем исходную высоту\n",
    "    resize_scale = IMG_HEIGHT / old_height       # считаем коэффициент масштабирования изображения до требуемого\n",
    "    img = scale_image(image=image, scale=resize_scale)\n",
    "    new_width = img.shape[1]      # получаем новую ширину\n",
    "    \n",
    "    # обрезаем паспорт до страницы с фото\n",
    "    x0 = INDENT_LEFT                            # отступ слева, т.к. корочка и фото нам не важны\n",
    "    y0 = IMG_HEIGHT // 2 + INDENT_TOP           # обрезка сверху, т.к. верхняя страница с местом выдачи нам не важна \n",
    "    x1 = new_width if new_width < IMG_WIDTH else IMG_WIDTH   # обрезаем все лишнее справа, если есть разворот с пропиской\n",
    "    y1 = IMG_HEIGHT - INDENT_BOTTOM\n",
    "    img = img[y0:y1, x0:x1]              # сохраняем вырезанный кусок изображения для передачи\n",
    "    \n",
    "    return img\n",
    "\n",
    "def set_gamma_corr(image, gamma=0.4):\n",
    "    # gamma-correction:\n",
    "    # O = ((I / 255) ** gamma )* 255\n",
    "    # when gamma [0.0-100.0] and if gamma < 0 then dark regions will be brighter and has more detale\n",
    "    lookup_table = np.empty((1,256), np.uint8)\n",
    "    for i in range(256):\n",
    "        lookup_table[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
    "    return cv2.LUT(image, lookup_table)\n",
    "\n",
    "    # The function addWeighted calculates the weighted sum of two arrays\n",
    "    #cal = cv2.addWeighted(cal, Alpha, cal, 0, Gamma)\n",
    "    \n",
    "\n",
    "\n",
    "def set_bright_cont(image, alpha=1.0, beta=0):\n",
    "    # when `alpha` [1.0-3.0] is contrast scale and `beta` [0-100] is brightness scale\n",
    "    # g(i,j) = alpha * g(i,j) + beta - for every pixels of image:\n",
    "    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "\n",
    "def normalize_color(image):         # принимаем объект изображения OpenCV\n",
    "    # Подготовка изображений для распознавания текста\n",
    "    # обесцвечиваем, если картинка цветная\n",
    "    if len(image.shape) > 2:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)    # преобразуем в ЧБ\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # Размытие для снижения количества шумов. Эксперимент показал, что без него буквы детектируются лучше\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)         # коэффициент размытия подобран вручную\n",
    "    \n",
    "    # Очередность преобраозвания найдена опытным путем\n",
    "    # TODO - при распознавании преобразуем в uint32, возможно и здесь стоит\n",
    "    kernel_block = np.ones((5,5), 'uint8')\n",
    "    kernel_gray = np.ones((3,3), 'uint8')  \n",
    "    # kernel = np.ones((5,5), 'uint16')   # не дал улучшения результата. при 'uint32' - код падает из-за ошибки разрешения\n",
    "    # kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))  # не знаю в чем разница, но так работает хуже\n",
    "    \n",
    "    # В теории erode - делает буквы тоньше, а dilate - толще: https://docs.opencv.org/3.4/db/df6/tutorial_erosion_dilatation.html\n",
    "    img_block = cv2.erode(gray, kernel_block, iterations=1)   # Но на практике \"жирность\" букв при этой операции повышается\n",
    "    # img_block = cv2.dilate(gray, kernel_gray, iterations=1)  # А тут - наоборот\n",
    "    # gray = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "    # gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 33, 15)\n",
    "    # _, gray = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "    \n",
    "    # TODO - поиграться с настройками, чтобы выдавать на выход именно контраст. Сейчас это только для детекции границ букв\n",
    "    _, img_block = cv2.threshold(img_block, 0, 255, cv2.THRESH_OTSU, cv2.THRESH_BINARY_INV) # Повышаем контраст\n",
    "    img_block = cv2.morphologyEx(img_block, cv2.MORPH_OPEN, kernel_block, iterations=1) # Снижаем шум на фоне\n",
    "    # img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Попытка найти лучший вариант детекции и выходного изображения. Оставил для дальнейших тестов\n",
    "    # Grayscale, Gaussian blur, Otsu's threshold\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Morph open to remove noise and invert image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    closing = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    erosion = cv2.erode(gray, kernel, iterations = 1)\n",
    "    dilation = cv2.dilate(gray, kernel, iterations = 1)\n",
    "    invert = 255 - closing\n",
    "    \n",
    "    # Повышение контраста\n",
    "    if len(image.shape) > 2:\n",
    "        imghsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        imghsv[:,:,2] = [[max(pixel - 25, 0) if pixel < 190 else min(pixel + 25, 255) for pixel in row] for row in imghsv[:,:,2]]\n",
    "        contrast = cv2.cvtColor(imghsv, cv2.COLOR_HSV2BGR)\n",
    "        gray_contrast = cv2.cvtColor(contrast, cv2.COLOR_BGR2GRAY)    # преобразуем в ЧБ\n",
    "        \n",
    "    # при коэффициенте 3 - лучше распознается Васлевский, при 5 - Соколов и Юмакаева\n",
    "    img_symbol = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 3, 2)\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_TOZERO+cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \"\"\"\n",
    "    return img_block, gray      # Возвращаем контрастную картинку с разбивкой на блоки и простое ЧБ изображение\n",
    "\n",
    "\n",
    "def search_blocks(image, limit: int, sort_by: str, sort_reverse=False):\n",
    "    # Выделяем элементы текста из изображения\n",
    "    #::limit:: - необходим чтобы указать на сколько мелкие символы нам не нужно распознавать\n",
    "    \n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    # получаем контуры больших пятен на изображении, внутри которых спрятан текст\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    # contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) # другая вариация\n",
    "    \n",
    "    # print(f'Count of Block counoturs: {len(contours)}')\n",
    "    blocks = []\n",
    "    for idx, contour in enumerate(contours):\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        # print(\"R\", x, y, w, h, hierarchy[0][idx])\n",
    "        # hierarchy[i][0]: следующий контур текущего уровня\n",
    "        # hierarchy[i][1]: предыдущий контур текущего уровня\n",
    "        # hierarchy[i][2]: первый вложенный элемент\n",
    "        # hierarchy[i][3]: родительский элемент\n",
    "        # if hierarchy[0][idx][3] == 0:               # если элемент не является самым крупным\n",
    "        # cv2.rectangle(image, (x, y), (x + w, y + h), (70, 0, 0), 1) # для контрольной картинки\n",
    "        \n",
    "        if limit < h < height and limit < w < width:    # игнорируем маленькие блоки, а также блок размером с изображение\n",
    "            block = image[y:y + h, x:x + w]     # вырезаем найденный блок из изображения\n",
    "            \n",
    "            # сохраняем габариты и изображение блока в список блоков. Загоняем в словарь, чтобы проще сортировать\n",
    "            # todo: По 'x' мы определяем очередность букв, ведь чем \"левее буква\", тем меньше ее 'x'. Также можно по 'y'\n",
    "            blocks.append({'idx': idx, 'y': y, 'h': h, 'x': x, 'w': w, 'block': block})\n",
    "    \n",
    "    # Сортируем по нужному ключу: 'y' для вертикали или 'x' по горизонтали. Так же можно и по индексу или размерам\n",
    "    blocks.sort(key=lambda x: x.get(sort_by), reverse=sort_reverse)\n",
    "    # print(blocks)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def img_cut_blocks(image):\n",
    "    \"\"\"\n",
    "    Больше не используется. Заменен функцией size_cut_blocks(), которая вместо изображения буквы принимает габариты буквы\n",
    "    из которых уже определяет нужно ли делить ее на части. Дополнительно увеличивает зону вырезки буквы,\n",
    "    чтобы добавить вокруг него пустое поле.\n",
    "    \"\"\"\n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    C = 1.2       # просто коэффициент, рассчитанный на широкие буквы вроде Ж, М, Ш и т.д., чтобы их не резало\n",
    "    if width < height*C:\n",
    "        # print(f'One symbol is True')\n",
    "        return [image]\n",
    "    else:\n",
    "        # нам нужно увеличить 'h' и 'w' на 2 размера margin, после чего уменьшить 'x' и 'y' на один размер margin\n",
    "        result = []\n",
    "        y, h = 0, height      # высота и верхняя точке среза - всегда неизменны\n",
    "        symbol_count = math.ceil(width / height)    # округляем символы до большего целого\n",
    "        symbol_width = math.floor(width / symbol_count)   # округляем ширину в пикселях до меньшего целого\n",
    "        \n",
    "        for i in range(symbol_count):\n",
    "            x = i * symbol_width\n",
    "            result.append(image[y:h, x:x+symbol_width])\n",
    "            # print(f'y = {y}, h = {h}, x = {x}, symbol_width = {x+symbol_width}, width = {width}')\n",
    "            # print(f'symbol {i} is:\\n{result[i]}')\n",
    "            \n",
    "        # print(f'Count of separeted symbols: {len(result)}')\n",
    "        return result\n",
    "\n",
    "    \n",
    "def size_cut_blocks(x: int, w: int, y: int, h: int, C=1.2, margin_x=SYMBOLS_MARGIN_X, margin_y=SYMBOLS_MARGIN_Y) -> dict:\n",
    "    \"\"\"\n",
    "    Если детектор букв выдает нам слишком \"широкий\" блок - значит он склеил несколько соседних букв.\n",
    "    Это возможно по двум причинам:\n",
    "    1. Плохое качество печати/изображения, тогда действительно соседние буквы сливаются даже для человеческого взгляда.\n",
    "    2. Плохое качество фильтра определения границ букв. С этим еще нужно поработать - поковырять параметры в normalize_color()\n",
    "    Выход - решать проблему математически (на вскидку). Если ширина больше высоты на определенную константу (подобрана руками)\n",
    "    то делим изображение на расчетное количество элементов.\n",
    "    Это не панацея, т.к. в зависимости от шрифта буква \"Ж\" может быть шире, чем сочетание \"СТ\". В таком случае при слишком\n",
    "    малом коэффициенте - \"Ж\" разделится на 2 буквы, а при слишком большом - \"СТ\" останется склеенным.\n",
    "    С английскими \"ij\" ситуация обстоит еще хуже.\n",
    "    \"\"\"\n",
    "    # print([x, w, y, h])\n",
    "    \n",
    "    # высчитываем новые опорные точки изображения с учетом полей\n",
    "    symbols_count = math.ceil(w / (h*C))    # округляем символы до большего целого\n",
    "    #symbol_width = math.floor(w / symbols_count)   # округляем ширину в пикселях до меньшего целого\n",
    "    symbol_width = math.ceil(w / symbols_count)   # округляем ширину в пикселях до большего целого\n",
    "    \n",
    "    new_x = x - int(symbol_width * margin_x)      # новый 'x' - это старый, уменьшенный на размер поля = 7.5% от ширины\n",
    "    new_w = symbol_width + int(symbol_width * margin_x * 2)   # новый 'w' - это старый, увеличенный на двойной размер поля\n",
    "    new_y = y - int(h * margin_y)                 # то же что 'x'\n",
    "    new_h = h + int(h * margin_y * 2)             # то же что 'w'\n",
    "    result = [[new_x], new_w, new_y, new_h]\n",
    "    \n",
    "    for i in range(1, symbols_count):     # цикл для расчета всех следующих 'x' в широких блоках\n",
    "        # x += i * symbol_width         # для 0-го символа 'x' не меняется, для последующих увеличивается на расчетную ширину\n",
    "        new_x = x + (i * symbol_width) - int(symbol_width * margin_x)   # новый 'x' - это старый, уменьшенный на размер поля = 7.5% от ширины\n",
    "        result[0].append(new_x)\n",
    "    \n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def labels_to_int(labels=LABELS) -> dict:\n",
    "    # Преобразуем букву в числвой код для создания классов в модели\n",
    "    # TODO - можно вынести этот функционал в препроцессинговый слой модели - layers.StringLookup(output_mode=\"one_hot\")\n",
    "    label_nums = {}\n",
    "    for i, lab in enumerate(labels):\n",
    "        label_nums[lab] = i\n",
    "    print(label_nums)\n",
    "    return label_nums\n",
    "\n",
    "\n",
    "labels_comparison = labels_to_int()     # получаем сопоставление символа к коду из датасета\n",
    "labels_symbol = list(labels_comparison.keys())     # и получаем отдельно список по символам и кодам\n",
    "labels_class = list(labels_comparison.values())\n",
    "\n",
    "\n",
    "def predict_img(model, img):\n",
    "    \n",
    "    img = img.reshape((1, 28, 28, 1))\n",
    "\n",
    "    predict = model.predict(img)\n",
    "    result = np.argmax(predict, axis=1)     # получаем индекс класса с наибольшей предсказанной вероятностью\n",
    "    \n",
    "    #TODO изменить на использование словаря соответствия класса и буквы. сейчас тупо по индексу класса забираю букву\n",
    "    #labels_symbol = list(labels_comparison.keys())\n",
    "    \n",
    "    print(labels_symbol[result[0]])\n",
    "    return labels_symbol[result[0]]\n",
    "\n",
    "\n",
    "def recognize_word(word_images_paths):\n",
    "    # Эта функция целиком слово по заданному пути к изображению слова.\n",
    "    # В текущей редакции код использует вместо этой функцию 'recognize_symbol'.\n",
    "    word_images = []     # сюда собираем список всех картинок для одного слова\n",
    "    predicted_word = ''\n",
    "\n",
    "    # Если только что занимались созданием модели - будет запущена она. Если нет - будет запущена версия с диска  \n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        print(\"The model just created will be used.\")\n",
    "    else:\n",
    "        try:\n",
    "            print(\"The Model has not been created in the current session. Loading the saved model.\")\n",
    "            model = keras.models.load_model(MODEL_PATH)\n",
    "        except NameError as ne:\n",
    "            print(ne)\n",
    "        except Exception as e:     # TODO Добавить обработку отсутствия файла\n",
    "            print(e)\n",
    "\n",
    "    # TODO - Переписать под использование картинок из кода выше, вместо исползования сохраненных на диск\n",
    "    # Либо вынести в отдельную функцию забор картинок из папки, и в отдельную - запуск обработки полноценных изображений\n",
    "    for letter_path in word_images_paths:\n",
    "        #word_images.append(cv2.imread(letter_path))\n",
    "        with Image.open(letter_path) as image:       # открываем картинку по ссылке, преобразуем в массив\n",
    "            # img_to_arr = np.asarray(image)           # преобразуем загруженную картинку к необходимой Модели форме\n",
    "            # img_to_arr = np.asarray([img_to_arr])    # требуется именно такое двойное преобразование \n",
    "            img_to_arr = np.asarray([np.asarray(image)])    # ТАК НАДО!!! Такова форма модели, иначе не работает\n",
    "            print('\\n', img_to_arr.shape)\n",
    "            print(letter_path)\n",
    "            #word_images.append(np.asarray(image))\n",
    "\n",
    "            predicted_word += predict_img(model=model, img=img_to_arr)\n",
    "\n",
    "    print(predicted_word)\n",
    "    with open('task_3.txt', 'w') as f:\n",
    "        f.write(predicted_word)\n",
    "\n",
    "        \n",
    "def recognize_symbol(model, symbol_img):\n",
    "    img_to_arr = np.asarray([np.asarray(symbol_img)])    # ТАК НАДО!!! Такова форма модели, иначе не работает\n",
    "    print('\\n', img_to_arr.shape)\n",
    "\n",
    "    return predict_img(model=model, img=img_to_arr)\n",
    "\n",
    "\n",
    "def recognize_by_tesseract(img, lang=LANG, psm=PSM, oem=OEM) -> str:\n",
    "    \"\"\" Распознаем текст с помощью Tesseract OCR\n",
    "    В качестве изображения готовы принять объект PIL.Image, numpy.ndarray (при открытии через cv2.imread())\n",
    "    или текстовый путь к изображению.\n",
    "    \"\"\"\n",
    "    from PIL.JpegImagePlugin import JpegImageFile\n",
    "    from PIL.PngImagePlugin import PngImageFile\n",
    "    from numpy import ndarray\n",
    "    \n",
    "    config = f'--oem {oem} --psm {psm}'\n",
    "    \n",
    "    # В зависимости от переданного формата, приводим изображение к нужному для распознавания\n",
    "    if isinstance(img, (JpegImageFile, PngImageFile)):\n",
    "        # если передан объект типа PIL - используем его для распознавания\n",
    "        image = img\n",
    "        \n",
    "    elif isinstance(img, ndarray):\n",
    "        # если передан объект в формате numpy, значит использовался cv2, а значит необходимо преобразовать формат\n",
    "        if len(img.shape) > 2:\n",
    "            image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            image = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "    elif isinstance(img, str):\n",
    "        # если передана строка - вероятно это путь к файлу, а значит его необходимо загрузить\n",
    "        # TODO - добавить проверку на существование файла\n",
    "        image = Image.open(img)\n",
    "    else:\n",
    "        # TODO - заменить вывод текста на возврат ошибки и ее обработку.\n",
    "        return f\"\"\"\n",
    "        The function expects an PIL image, OpenCV image (numpy.ndarray) or a path to a file in a text format.\n",
    "        Instead, another object is received. Check the passed value.\n",
    "        \"\"\"\n",
    "    \n",
    "    return pytesseract.image_to_string(image, lang=lang, config=config) + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24aaac0",
   "metadata": {},
   "source": [
    "## Готовим модель и train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгатавливаем модель для распознавания букв из датасетов по аналогии с EMNIST\n",
    "\n",
    "# TODO - вместо подгонки данных перед отправкой на распознавание, нужно добавить этот слой непосредственно в модель\n",
    "\"\"\"\n",
    "resize_and_rescale = Sequential([\n",
    "    layers.Resizing(DATASET_SYMBOL_SIZE, DATASET_SYMBOL_SIZE),\n",
    "    layers.Rescaling(1./255)\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "# does not use. TODO - переписать модель под использование этой функции\n",
    "def resize_and_rescale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [DATASET_SYMBOL_SIZE, DATASET_SYMBOL_SIZE])\n",
    "    image = (image / 255.0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# does not use.\n",
    "def augment(image, label, seed=0):\n",
    "    #image, label = image_label\n",
    "    #image, label = resize_and_rescale(image, label)\n",
    "    #image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE + 6, IMG_SIZE + 6)\n",
    "    # Make a new seed.\n",
    "    new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]\n",
    "    # Random crop back to the original size.\n",
    "    #image = tf.image.stateless_random_crop(\n",
    "    #  image, size=[IMG_SIZE, IMG_SIZE, 3], seed=seed)\n",
    "    # Random brightness.\n",
    "    image = tf.image.stateless_random_brightness(\n",
    "      image, max_delta=0.5, seed=new_seed)\n",
    "    # Random contrast.\n",
    "    image = tf.image.stateless_random_contrast(\n",
    "      image, lower=0.1, upper=0.9, seed=new_seed)\n",
    "    image = tf.clip_by_value(image, 0, 1)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# does not use. Аугментация не нужна, если мы нормализуем изображения на входе в нейронку\n",
    "data_augmentation = Sequential([\n",
    "    RandomContrast(factor=0.25), # +/- 25% к контрастру тестовых изображений\n",
    "    RandomRotation(factor=0.02)  # +/- 0.02 * 2pi градусов к повороту тестовых изображений ~7.2 градуса\n",
    "    # RandomBrightness(factor=0.5),\n",
    "    # RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "\n",
    "def main_model(img_size=DATASET_SYMBOL_SIZE, lb_count=len(LABELS)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Предобработка изображений для обучения и распознавания модели. Это нужно, чтобы не подгонять до отправки в модель\n",
    "    #model.add(Resizing(img_size, img_size))\n",
    "    model.add(Rescaling(1./255))  # если тут мы раскомментим, тренировочные и предсказываемые данные будут нормализованы\n",
    "    #model.add(Rescaling(scale=1./127.5, offset=-1))  # сравнить с такой нормализацией\n",
    "    \n",
    "    # Аугментация тестовых данных. Преобразование цвета не имеет смысла, если мы нормализуем входящие изображения\n",
    "    # model.add(data_augmentation)  # TODO - использовать вместо этой строки - две строки ниже\n",
    "    # model.add(RandomContrast(factor=0.25)) # Аугментация контраста +/- 50%. В теории вместо этого можно нормализвать изображение\n",
    "    model.add(RandomRotation(factor=0.02, fill_mode=\"nearest\")) # 0.02 * 2pi градусов к повороту тестовых изображений, ~7.2 градуса\n",
    "    \n",
    "    # Основная модель\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='valid',\n",
    "                            input_shape=(img_size, img_size, 1), activation='relu'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(lb_count, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae630dd1",
   "metadata": {},
   "source": [
    "## Загружаем датасет и разбиваем на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10173127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (125000, 28, 28)\n",
      "X_valid: (25000, 28, 28)\n",
      "X_test: (25000, 28, 28)\n",
      "y_train: (125000,)\n",
      "y_valid: (25000,)\n",
      "y_test: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Загружаем датасет Часть 1 / 2\n",
    "TRAIN_SHARE = 0.7       # доля тренировочных данных = 75%\n",
    "VALID_SHARE = (1 - TRAIN_SHARE) / 2     # доля валидационных данных - это половина от 100% минус тренировочные данные\n",
    "\n",
    "# Загружаем датасеты картинок и их классов\n",
    "#ds_images = idx.convert_from_file(DATASET_IMG)\n",
    "#ds_classes = idx.convert_from_file(DATASET_CLS)\n",
    "\n",
    "#train_slice = int(TRAIN_SHARE*len(ds_images))   # получаем границу для вырезки тренировочных данных\n",
    "#valid_slice = train_slice + int(VALID_SHARE*len(ds_images))  # получаем границу для вырезки валидационных данных\n",
    "#X_train, X_valid, X_test = np.split(ds_images, [train_slice, valid_slice])\n",
    "#y_train, y_valid, y_test = np.split(ds_classes, [train_slice, valid_slice])\n",
    "\n",
    "\n",
    "# Датасет с разбивкой на train, valid и test\n",
    "X_train = idx.convert_from_file(DS_TRAIN_IMG)\n",
    "y_train = idx.convert_from_file(DS_TRAIN_LBL)\n",
    "X_valid = idx.convert_from_file(DS_VALID_IMG)\n",
    "y_valid = idx.convert_from_file(DS_VALID_LBL)\n",
    "X_test = idx.convert_from_file(DS_TEST_IMG)\n",
    "y_test = idx.convert_from_file(DS_TEST_LBL)\n",
    "\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_valid: {X_valid.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_valid: {y_valid.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "\n",
    "#print(train_slice)\n",
    "#print(valid_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620d3ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000, 28, 28, 1) (125000,) (25000, 28, 28, 1) (25000,) (25000, 28, 28, 1) (25000,) 33\n",
      "(125000, 33) (25000, 33) (25000, 33)\n"
     ]
    }
   ],
   "source": [
    "# Загружаем датасет Часть 2 / 2\n",
    "\n",
    "# зачем-то предлагают сделать решейп датасета картинок, добавляя к слою изображения еще одно измерение\n",
    "# такой же решейп предлагается для отправляемого в готовую модель изображения. хз надо ли это делать тут и там\n",
    "# TODO - заменить предварительную обработку изображений на  предобработку непосредственно в первых слоях модели\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1))\n",
    "X_valid = np.reshape(X_valid, (X_valid.shape[0], 28, 28, 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Это тупо уменьшаем выборку из датасетов в 10 раз\n",
    "#k = 10\n",
    "#X_train = X_train[:X_train.shape[0] // k]\n",
    "#y_train = y_train[:y_train.shape[0] // k]\n",
    "#X_test = X_test[:X_test.shape[0] // k]\n",
    "#y_test = y_test[:y_test.shape[0] // k]\n",
    "\n",
    "# Нормализация изображений в датасете. Заменяет нам корректировку яркости и контраста. Темное к 0.0, светлое - к 1.0\n",
    "# Реализовал нормализацию в первом слое самой модели, поэтому предварительная операция не требуется\n",
    "\"\"\"\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_train /= 255.0\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "X_valid /= 255.0\n",
    "X_test = X_test.astype(np.float32)\n",
    "X_test /= 255.0\n",
    "\"\"\"\n",
    "\n",
    "# \"маска\" на которую будут созданы предсказание категорий\n",
    "# TODO - перенести преобразование y_train/y_test к категорийному представлению - в слои препроцессинга модели\n",
    "# TODO - можно пересохранить y_train, y_valid, y_test вместо создания новых категорийных переменных\n",
    "y_train_cat = keras.utils.to_categorical(y_train, len(labels_comparison))    ### Вот так распознавание работает!!!\n",
    "y_valid_cat = keras.utils.to_categorical(y_valid, len(labels_comparison))\n",
    "y_test_cat = keras.utils.to_categorical(y_test, len(labels_comparison))\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape, len(labels_comparison))\n",
    "print(y_train_cat.shape, y_valid_cat.shape, y_test_cat.shape)\n",
    "\n",
    "# Генератор аугментации для добавления разных вариантов яркости и контраста в изображения\n",
    "# TODO - этот класс `depricated`, поэтому необходимо переписать аугментацию новым способом (с добавлением в модель)\n",
    "# Кроме того эксперименты показали, что его использование сводит на нет сходимость модели. То ли я неверно его использую,\n",
    "# то ли оно работает плохо.\n",
    "datagen = ImageDataGenerator(\n",
    "    brightness_range=[0.2, 1.2], # Диапазон яркости\n",
    "    #rescale=1/255.,   # тут это не нужно, т.к. уже делаем нормализацию при загрузке Датасета.\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    #rotation_range=5,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2,\n",
    "    #validation_split=0.15)   # Вероятно, аугментация валидационных данных приводит к тому, что модель не сходится\n",
    ")\n",
    "#datagen_train = datagen.flow(X_train, y_train_cat, subset='training')\n",
    "#dataget_valid = datagen.flow(X_train, y_train_cat, subset='validation', class_mode='categorical') # class_mode доступен для .fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a74b07",
   "metadata": {},
   "source": [
    "## Обучаем модель на датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ff8226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Start Datagen.fit() ======\n",
      "====== Start Model.fit() ======\n",
      "Epoch 1/150\n",
      "1954/1954 [==============================] - 160s 81ms/step - loss: 3.1818 - accuracy: 0.1468 - val_loss: 2.9191 - val_accuracy: 0.2613 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 2.8474 - accuracy: 0.2676 - val_loss: 2.5371 - val_accuracy: 0.4505 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "1954/1954 [==============================] - 181s 92ms/step - loss: 2.4558 - accuracy: 0.4136 - val_loss: 1.9957 - val_accuracy: 0.6346 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "1954/1954 [==============================] - 179s 91ms/step - loss: 1.9624 - accuracy: 0.5496 - val_loss: 1.4363 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "1954/1954 [==============================] - 182s 93ms/step - loss: 1.4965 - accuracy: 0.6652 - val_loss: 0.9840 - val_accuracy: 0.8249 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "1954/1954 [==============================] - 183s 94ms/step - loss: 1.1360 - accuracy: 0.7450 - val_loss: 0.6832 - val_accuracy: 0.8687 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "1954/1954 [==============================] - 181s 93ms/step - loss: 0.8758 - accuracy: 0.7988 - val_loss: 0.4880 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "1954/1954 [==============================] - 179s 92ms/step - loss: 0.6985 - accuracy: 0.8353 - val_loss: 0.3726 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.5772 - accuracy: 0.8588 - val_loss: 0.2974 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "1954/1954 [==============================] - 175s 89ms/step - loss: 0.4885 - accuracy: 0.8760 - val_loss: 0.2431 - val_accuracy: 0.9296 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "1954/1954 [==============================] - 175s 90ms/step - loss: 0.4183 - accuracy: 0.8897 - val_loss: 0.1994 - val_accuracy: 0.9516 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "1954/1954 [==============================] - 175s 89ms/step - loss: 0.3663 - accuracy: 0.8999 - val_loss: 0.1684 - val_accuracy: 0.9517 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "1954/1954 [==============================] - 175s 90ms/step - loss: 0.3187 - accuracy: 0.9105 - val_loss: 0.1433 - val_accuracy: 0.9529 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.2844 - accuracy: 0.9184 - val_loss: 0.1183 - val_accuracy: 0.9586 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "1954/1954 [==============================] - 177s 90ms/step - loss: 0.2532 - accuracy: 0.9257 - val_loss: 0.1029 - val_accuracy: 0.9602 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "1954/1954 [==============================] - 184s 94ms/step - loss: 0.2256 - accuracy: 0.9322 - val_loss: 0.0876 - val_accuracy: 0.9617 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "1954/1954 [==============================] - 183s 93ms/step - loss: 0.2043 - accuracy: 0.9396 - val_loss: 0.0761 - val_accuracy: 0.9660 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "1954/1954 [==============================] - 177s 91ms/step - loss: 0.1843 - accuracy: 0.9445 - val_loss: 0.0663 - val_accuracy: 0.9687 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "1954/1954 [==============================] - 179s 92ms/step - loss: 0.1674 - accuracy: 0.9495 - val_loss: 0.0569 - val_accuracy: 0.9829 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.1527 - accuracy: 0.9535 - val_loss: 0.0502 - val_accuracy: 0.9909 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "1954/1954 [==============================] - 181s 93ms/step - loss: 0.1393 - accuracy: 0.9580 - val_loss: 0.0459 - val_accuracy: 0.9863 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "1954/1954 [==============================] - 179s 91ms/step - loss: 0.1282 - accuracy: 0.9620 - val_loss: 0.0393 - val_accuracy: 0.9981 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "1954/1954 [==============================] - 179s 92ms/step - loss: 0.1179 - accuracy: 0.9650 - val_loss: 0.0356 - val_accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "1954/1954 [==============================] - 175s 90ms/step - loss: 0.1083 - accuracy: 0.9683 - val_loss: 0.0315 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.1000 - accuracy: 0.9713 - val_loss: 0.0281 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "1954/1954 [==============================] - 181s 93ms/step - loss: 0.0932 - accuracy: 0.9731 - val_loss: 0.0255 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0850 - accuracy: 0.9758\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0851 - accuracy: 0.9758 - val_loss: 0.0227 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "1954/1954 [==============================] - 179s 92ms/step - loss: 0.0807 - accuracy: 0.9781 - val_loss: 0.0213 - val_accuracy: 0.9992 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0790 - accuracy: 0.9783 - val_loss: 0.0202 - val_accuracy: 0.9992 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9795\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0757 - accuracy: 0.9795 - val_loss: 0.0190 - val_accuracy: 0.9992 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0741 - accuracy: 0.9796 - val_loss: 0.0187 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 32/150\n",
      "1954/1954 [==============================] - 181s 93ms/step - loss: 0.0726 - accuracy: 0.9803 - val_loss: 0.0182 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 33/150\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9816\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0704 - accuracy: 0.9816 - val_loss: 0.0176 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 34/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0706 - accuracy: 0.9809 - val_loss: 0.0176 - val_accuracy: 0.9992 - lr: 1.2500e-04\n",
      "Epoch 35/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0687 - accuracy: 0.9818 - val_loss: 0.0173 - val_accuracy: 0.9992 - lr: 1.2500e-04\n",
      "Epoch 36/150\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9821\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0680 - accuracy: 0.9821 - val_loss: 0.0169 - val_accuracy: 0.9992 - lr: 1.2500e-04\n",
      "Epoch 37/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0688 - accuracy: 0.9815 - val_loss: 0.0168 - val_accuracy: 0.9992 - lr: 6.2500e-05\n",
      "Epoch 38/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0675 - accuracy: 0.9828 - val_loss: 0.0167 - val_accuracy: 0.9992 - lr: 6.2500e-05\n",
      "Epoch 39/150\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9827\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0670 - accuracy: 0.9827 - val_loss: 0.0166 - val_accuracy: 0.9992 - lr: 6.2500e-05\n",
      "Epoch 40/150\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0671 - accuracy: 0.9825 - val_loss: 0.0165 - val_accuracy: 0.9992 - lr: 3.1250e-05\n",
      "Epoch 41/150\n",
      "1954/1954 [==============================] - 182s 93ms/step - loss: 0.0667 - accuracy: 0.9825 - val_loss: 0.0165 - val_accuracy: 0.9992 - lr: 3.1250e-05\n",
      "Epoch 42/150\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9832\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1954/1954 [==============================] - 177s 90ms/step - loss: 0.0658 - accuracy: 0.9832 - val_loss: 0.0164 - val_accuracy: 0.9992 - lr: 3.1250e-05\n",
      "Epoch 43/150\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0667 - accuracy: 0.9822 - val_loss: 0.0164 - val_accuracy: 0.9992 - lr: 1.5625e-05\n",
      "Epoch 44/150\n",
      "1954/1954 [==============================] - 175s 90ms/step - loss: 0.0668 - accuracy: 0.9825 - val_loss: 0.0164 - val_accuracy: 0.9992 - lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/150\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9824\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "1954/1954 [==============================] - 177s 91ms/step - loss: 0.0670 - accuracy: 0.9824 - val_loss: 0.0164 - val_accuracy: 0.9992 - lr: 1.5625e-05\n",
      "Epoch 46/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0667 - accuracy: 0.9825 - val_loss: 0.0164 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 47/150\n",
      "1954/1954 [==============================] - 182s 93ms/step - loss: 0.0666 - accuracy: 0.9822 - val_loss: 0.0163 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 48/150\n",
      "1954/1954 [==============================] - 183s 94ms/step - loss: 0.0668 - accuracy: 0.9827 - val_loss: 0.0163 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 49/150\n",
      "1954/1954 [==============================] - 179s 92ms/step - loss: 0.0662 - accuracy: 0.9827 - val_loss: 0.0163 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 50/150\n",
      "1954/1954 [==============================] - 182s 93ms/step - loss: 0.0658 - accuracy: 0.9830 - val_loss: 0.0163 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 51/150\n",
      "1954/1954 [==============================] - 183s 94ms/step - loss: 0.0665 - accuracy: 0.9824 - val_loss: 0.0163 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 52/150\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0656 - accuracy: 0.9828 - val_loss: 0.0162 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 53/150\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0664 - accuracy: 0.9823 - val_loss: 0.0162 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 54/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0665 - accuracy: 0.9826 - val_loss: 0.0162 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 55/150\n",
      "1954/1954 [==============================] - 177s 91ms/step - loss: 0.0666 - accuracy: 0.9827 - val_loss: 0.0162 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 56/150\n",
      "1954/1954 [==============================] - 182s 93ms/step - loss: 0.0663 - accuracy: 0.9828 - val_loss: 0.0162 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 57/150\n",
      "1954/1954 [==============================] - 181s 93ms/step - loss: 0.0658 - accuracy: 0.9828 - val_loss: 0.0162 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 58/150\n",
      "1954/1954 [==============================] - 179s 91ms/step - loss: 0.0673 - accuracy: 0.9821 - val_loss: 0.0162 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 59/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0660 - accuracy: 0.9828 - val_loss: 0.0161 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 60/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0663 - accuracy: 0.9829 - val_loss: 0.0161 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 61/150\n",
      "1954/1954 [==============================] - 183s 94ms/step - loss: 0.0653 - accuracy: 0.9831 - val_loss: 0.0161 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 62/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0649 - accuracy: 0.9831 - val_loss: 0.0161 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 63/150\n",
      "1954/1954 [==============================] - 179s 91ms/step - loss: 0.0663 - accuracy: 0.9829 - val_loss: 0.0161 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 64/150\n",
      "1954/1954 [==============================] - 177s 91ms/step - loss: 0.0647 - accuracy: 0.9836 - val_loss: 0.0161 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 65/150\n",
      "1954/1954 [==============================] - 177s 91ms/step - loss: 0.0658 - accuracy: 0.9829 - val_loss: 0.0160 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 66/150\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0658 - accuracy: 0.9824 - val_loss: 0.0160 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 67/150\n",
      "1954/1954 [==============================] - 177s 91ms/step - loss: 0.0663 - accuracy: 0.9825 - val_loss: 0.0160 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 68/150\n",
      "1954/1954 [==============================] - 181s 92ms/step - loss: 0.0660 - accuracy: 0.9830 - val_loss: 0.0160 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 69/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0663 - accuracy: 0.9827 - val_loss: 0.0160 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 70/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0656 - accuracy: 0.9826 - val_loss: 0.0160 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 71/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0651 - accuracy: 0.9831 - val_loss: 0.0159 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 72/150\n",
      "1954/1954 [==============================] - 179s 92ms/step - loss: 0.0659 - accuracy: 0.9826 - val_loss: 0.0159 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 73/150\n",
      "1954/1954 [==============================] - 179s 92ms/step - loss: 0.0650 - accuracy: 0.9830 - val_loss: 0.0159 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 74/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0652 - accuracy: 0.9833 - val_loss: 0.0159 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 75/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0660 - accuracy: 0.9825 - val_loss: 0.0159 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 76/150\n",
      "1954/1954 [==============================] - 177s 91ms/step - loss: 0.0654 - accuracy: 0.9833 - val_loss: 0.0159 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 77/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0658 - accuracy: 0.9826 - val_loss: 0.0159 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 78/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0651 - accuracy: 0.9830 - val_loss: 0.0158 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 79/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0648 - accuracy: 0.9834 - val_loss: 0.0158 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 80/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0653 - accuracy: 0.9824 - val_loss: 0.0158 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 81/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0643 - accuracy: 0.9832 - val_loss: 0.0158 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 82/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0654 - accuracy: 0.9828 - val_loss: 0.0158 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 83/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0649 - accuracy: 0.9833 - val_loss: 0.0157 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 84/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0658 - accuracy: 0.9824 - val_loss: 0.0157 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 85/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0638 - accuracy: 0.9835 - val_loss: 0.0157 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 86/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0639 - accuracy: 0.9838 - val_loss: 0.0157 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 87/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0658 - accuracy: 0.9825 - val_loss: 0.0157 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 88/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0646 - accuracy: 0.9834 - val_loss: 0.0157 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 89/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0655 - accuracy: 0.9829 - val_loss: 0.0156 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 90/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0646 - accuracy: 0.9835 - val_loss: 0.0156 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 91/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0642 - accuracy: 0.9835 - val_loss: 0.0156 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 92/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0648 - accuracy: 0.9834 - val_loss: 0.0156 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 93/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0648 - accuracy: 0.9834 - val_loss: 0.0156 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 94/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0645 - accuracy: 0.9831 - val_loss: 0.0156 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 95/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0638 - accuracy: 0.9834 - val_loss: 0.0155 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 96/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0645 - accuracy: 0.9832 - val_loss: 0.0155 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 97/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0643 - accuracy: 0.9832 - val_loss: 0.0155 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 98/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0636 - accuracy: 0.9835 - val_loss: 0.0155 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 99/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0640 - accuracy: 0.9835 - val_loss: 0.0155 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 100/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0646 - accuracy: 0.9832 - val_loss: 0.0155 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 101/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0640 - accuracy: 0.9831 - val_loss: 0.0154 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 102/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0641 - accuracy: 0.9839 - val_loss: 0.0154 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 103/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0644 - accuracy: 0.9832 - val_loss: 0.0154 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 104/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0642 - accuracy: 0.9835 - val_loss: 0.0154 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 105/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0639 - accuracy: 0.9835 - val_loss: 0.0154 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 106/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0631 - accuracy: 0.9837 - val_loss: 0.0154 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 107/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0643 - accuracy: 0.9833 - val_loss: 0.0154 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 108/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0635 - accuracy: 0.9838 - val_loss: 0.0153 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 109/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0638 - accuracy: 0.9833 - val_loss: 0.0153 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 110/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0643 - accuracy: 0.9833 - val_loss: 0.0153 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 111/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0635 - accuracy: 0.9834 - val_loss: 0.0153 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 112/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0153 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 113/150\n",
      "1954/1954 [==============================] - 175s 90ms/step - loss: 0.0633 - accuracy: 0.9838 - val_loss: 0.0153 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 114/150\n",
      "1954/1954 [==============================] - 172s 88ms/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0152 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 115/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0633 - accuracy: 0.9841 - val_loss: 0.0152 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 116/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0641 - accuracy: 0.9834 - val_loss: 0.0152 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 117/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0630 - accuracy: 0.9836 - val_loss: 0.0152 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 118/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0633 - accuracy: 0.9836 - val_loss: 0.0152 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 119/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0628 - accuracy: 0.9840 - val_loss: 0.0152 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 120/150\n",
      "1954/1954 [==============================] - 173s 88ms/step - loss: 0.0625 - accuracy: 0.9840 - val_loss: 0.0151 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 121/150\n",
      "1954/1954 [==============================] - 175s 89ms/step - loss: 0.0627 - accuracy: 0.9839 - val_loss: 0.0151 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 122/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0626 - accuracy: 0.9836 - val_loss: 0.0151 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 123/150\n",
      "1954/1954 [==============================] - 174s 89ms/step - loss: 0.0632 - accuracy: 0.9837 - val_loss: 0.0151 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 124/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0637 - accuracy: 0.9834 - val_loss: 0.0151 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 125/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0636 - accuracy: 0.9834 - val_loss: 0.0151 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 126/150\n",
      "1954/1954 [==============================] - 177s 90ms/step - loss: 0.0631 - accuracy: 0.9838 - val_loss: 0.0150 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 127/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0627 - accuracy: 0.9840 - val_loss: 0.0150 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 128/150\n",
      "1954/1954 [==============================] - 173s 89ms/step - loss: 0.0623 - accuracy: 0.9839 - val_loss: 0.0150 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 129/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0628 - accuracy: 0.9841 - val_loss: 0.0150 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 130/150\n",
      "1954/1954 [==============================] - 179s 92ms/step - loss: 0.0630 - accuracy: 0.9839 - val_loss: 0.0150 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 131/150\n",
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0626 - accuracy: 0.9846 - val_loss: 0.0150 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 132/150\n",
      "1954/1954 [==============================] - 177s 90ms/step - loss: 0.0626 - accuracy: 0.9840 - val_loss: 0.0150 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 133/150\n",
      "1954/1954 [==============================] - 170s 87ms/step - loss: 0.0625 - accuracy: 0.9844 - val_loss: 0.0150 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 134/150\n",
      "1954/1954 [==============================] - 139s 71ms/step - loss: 0.0624 - accuracy: 0.9835 - val_loss: 0.0149 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 135/150\n",
      "1954/1954 [==============================] - 148s 76ms/step - loss: 0.0635 - accuracy: 0.9836 - val_loss: 0.0149 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 136/150\n",
      "1954/1954 [==============================] - 158s 81ms/step - loss: 0.0627 - accuracy: 0.9839 - val_loss: 0.0149 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 137/150\n",
      "1954/1954 [==============================] - 168s 86ms/step - loss: 0.0621 - accuracy: 0.9844 - val_loss: 0.0149 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 138/150\n",
      "1954/1954 [==============================] - 163s 83ms/step - loss: 0.0618 - accuracy: 0.9842 - val_loss: 0.0149 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 139/150\n",
      "1954/1954 [==============================] - 170s 87ms/step - loss: 0.0628 - accuracy: 0.9840 - val_loss: 0.0148 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 140/150\n",
      "1954/1954 [==============================] - 171s 87ms/step - loss: 0.0618 - accuracy: 0.9842 - val_loss: 0.0148 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 141/150\n",
      "1954/1954 [==============================] - 177s 91ms/step - loss: 0.0624 - accuracy: 0.9839 - val_loss: 0.0148 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1954/1954 [==============================] - 176s 90ms/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0148 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 143/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0622 - accuracy: 0.9838 - val_loss: 0.0148 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 144/150\n",
      "1954/1954 [==============================] - 181s 93ms/step - loss: 0.0632 - accuracy: 0.9838 - val_loss: 0.0148 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 145/150\n",
      "1954/1954 [==============================] - 183s 94ms/step - loss: 0.0617 - accuracy: 0.9841 - val_loss: 0.0147 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 146/150\n",
      "1954/1954 [==============================] - 183s 94ms/step - loss: 0.0622 - accuracy: 0.9843 - val_loss: 0.0147 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 147/150\n",
      "1954/1954 [==============================] - 181s 92ms/step - loss: 0.0626 - accuracy: 0.9837 - val_loss: 0.0147 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 148/150\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0620 - accuracy: 0.9841 - val_loss: 0.0147 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 149/150\n",
      "1954/1954 [==============================] - 178s 91ms/step - loss: 0.0628 - accuracy: 0.9840 - val_loss: 0.0147 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "Epoch 150/150\n",
      "1954/1954 [==============================] - 180s 92ms/step - loss: 0.0629 - accuracy: 0.9839 - val_loss: 0.0147 - val_accuracy: 0.9992 - lr: 1.0000e-05\n",
      "====== Start Model.evaluate() ======\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.0136 - accuracy: 0.9997\n",
      "\n",
      "Started at: 2022.12.25 01:58:38\n",
      "Finished at: 2022.12.25 09:17:28\n",
      "Total Runtime: 7:18:49.863004\n",
      "Loss: 0.01361\n",
      "Accuracy: 0.99968\n"
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "EPOCH = 55     # TODO - прописать функционал авто выхода из обучения, если качество не улучшается на протяжении 10 эпох\n",
    "DT_PATTERN = (\"%Y.%m.%d %H:%M:%S\")   # шаблон для вывода даты и времени запуска и завершения обучения модели\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Способ обучения модели по изображениям прямо в папке без создания отдельного датасета\n",
    "# В нем же задаются настройки аугментации, которая расширяет общую базу изображений с учетом модификаций картинки\n",
    "# Генерируем изображения\n",
    "generator = datagen.flow_from_directory(\n",
    "    'data/train', # Папка с изображениями для обучения\n",
    "    target_size=(224, 224), # Размер изображений\n",
    "    batch_size=BATCH, # Размер батча\n",
    "    class_mode='categorical' # Категориальная классификация\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "model.fit_generator(\n",
    "    generator,\n",
    "    steps_per_epoch=len(generator),\n",
    "    epochs=10\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# TODO - протестировать запуск на 100+ эпох 1/10 часть датасета (10 тыс изображений) с аугментацией\n",
    "\n",
    "# Set a learning rate reduction\n",
    "learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "# Загружаем в модель размер изображений и количество классов\n",
    "model = main_model(DATASET_SYMBOL_SIZE, len(labels_comparison))\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "\"\"\"\n",
    "model.fit(datagen.flow(X_train, y_train_cat, batch_size=BATCH,\n",
    "         subset='training'),\n",
    "         validation_data=datagen.flow(X_test, y_test_cat,\n",
    "         batch_size=BATCH, subset='validation'),\n",
    "         steps_per_epoch=len(X_train) // BATCH, epochs=EPOCH)\n",
    "\"\"\"\n",
    "print(\"====== Start Datagen.fit() ======\")\n",
    "# datagen.fit(X_train) # does not use\n",
    "print(\"====== Start Model.fit() ======\")\n",
    "\"\"\"\n",
    "model.fit(datagen.flow(X_train, y_train_cat, subset='training', batch_size=BATCH),\n",
    "          validation_data=datagen.flow(X_train, y_train_cat, subset='validation', batch_size=BATCH),\n",
    "          callbacks=[learning_rate_reduction], batch_size=BATCH, epochs=EPOCH)\n",
    "\"\"\"\n",
    "\n",
    "#model.fit(datagen.flow(X_train, y_train_cat, batch_size=BATCH), validation_data=(X_test, y_test_cat),\n",
    "#          callbacks=[learning_rate_reduction], batch_size=BATCH, epochs=EPOCH)\n",
    "\n",
    "# ТУТ ОТЛИЧНАЯ СКОРОСТЬ ОБУЧЕНИЯ ПРИ ИСПОЛЬЗОВАНИИ 2-Х АУГМЕНТИРУЮЩИХ СЛОЕВ В МОДЕЛИ\n",
    "model.fit(X_train, y_train_cat, validation_data=(X_valid, y_valid_cat),\n",
    "          callbacks=[learning_rate_reduction], batch_size=BATCH, epochs=EPOCH)\n",
    "\n",
    "\n",
    "# model.fit(X_train, y_train_cat, validation_data=(X_test, y_test_cat), callbacks=[learning_rate_reduction], batch_size=BATCH, epochs=EPOCH)\n",
    "\n",
    "print(\"====== Start Model.evaluate() ======\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test_cat, batch_size=BATCH)\n",
    "\n",
    "model_name = f'ru_letters_{int(X_train.shape[0]//1000)}k_b{BATCH}_e{EPOCH}_DS-FIX_loss_{round(loss, 2)}_acc_{round(accuracy, 2)}.h5'\n",
    "model.save(model_name)\n",
    "\n",
    "finish_time = datetime.now()\n",
    "runtime = finish_time - start_time\n",
    "\n",
    "time_log = f\"\"\"\n",
    "Started at: {start_time:{DT_PATTERN}}\n",
    "Finished at: {finish_time:{DT_PATTERN}}\n",
    "Total Runtime: {runtime}\n",
    "Loss: {round(loss, 5)}\n",
    "Accuracy: {round(accuracy, 5)}\"\"\"\n",
    "\n",
    "print(time_log)\n",
    "with open(f'{model_name}.txt', 'w') as f:\n",
    "    f.write(time_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb2ed8",
   "metadata": {},
   "source": [
    "## Детекция данных из паспорта и сохранение фоток букв в файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6382eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def passport_data_parser(work_dir: str, count=0, model=MODEL_PATH):\n",
    "    # Запускаем цикл по всем фото в рабочей папке\n",
    "    try:     # TODO добавить проверку на существование файла\n",
    "        passports = get_files(work_dir)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "     \n",
    "    export_words = []\n",
    "    if count > 0:\n",
    "        count = min(count, len(passports))     # Что меньше - по такой индекс и забираем фотки (для тестов)\n",
    "    else:\n",
    "        count = len(passports)\n",
    "    \n",
    "    for id_p, passport in enumerate(passports[:count]):     # идем по списку путей к изображениям (ограничив длину списка)\n",
    "        export_words.append([])                         # добавляем пустой список для заполнения данных паспорта\n",
    "        temp_dir = os.path.join(TEMP_DIR, str(id_p))\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.mkdir(temp_dir)                              # создаем папку для сохранения промежуточных картинок\n",
    "\n",
    "        print(f'==== Image {id_p}.jpg =====')\n",
    "        image = cut_passport_info_area(cv2.imread(passport))     # получаем кусок паспорта с ФИО\n",
    "        # imb_blocks - используем для детектирования слов, img_gray - используем для распознавания текста внутри блоков\n",
    "        img_blocks, img_gray = normalize_color(image=image)      \n",
    "        \n",
    "        # TODO - убрать сохранение промежутоных файлов, они используются только для визуального контроля\n",
    "        cv2.imwrite(f'{TEMP_DIR}/{id_p}_blocs.jpg', img_blocks)\n",
    "        cv2.imwrite(f'{TEMP_DIR}/{id_p}_symbols.jpg', img_gray)\n",
    "\n",
    "        words = search_blocks(image=img_blocks, limit=15, sort_by='y')   # ищем блоки на картинке с жирными буквами\n",
    "\n",
    "        # получаем все обнаруженные слова из файла, в котором читаются символы\n",
    "        for id_w, word in enumerate(words[:3]):    # можно забирать только первые 3 слова ФИО\n",
    "            export_words[id_p].append([])      # добавляем вложенный список для каждого слова\n",
    "            # из словаря обнаруженного блока текста забираем координаты и размер блока\n",
    "            y, h, x, w = word['y'], word['h'], word['x'], word['w']\n",
    "            \n",
    "            img_word = img_gray[y:y + h, x:x + w]     # вырезаем слово из серой картинки по его координатам\n",
    "            #img_word = cn.normalize_img_color(img_word)\n",
    "            img_word = scale_image(img_word, SCALE_FACTOR)   # увеличиваем изображение, чтобы детектировать буквы\n",
    "            \n",
    "            word_blocks, _ = normalize_color(image=img_word) # прогоняем через детектор увеличенное фото слова\n",
    "            symbols = search_blocks(image=word_blocks, limit=SCALE_FACTOR*10, sort_by='x')  # слева направо\n",
    "            \n",
    "            #thresh = cv2.adaptiveThreshold(img_word, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 11)\n",
    "            #symbols = search_blocks(image=thresh, limit=SCALE_FACTOR*10, sort_by='x')  # слева направо\n",
    "            \n",
    "            # TODO - частичный повтор кода. Придумать как переделать, чтобы не дублировать функционал\n",
    "            # При распознавании через Tesseract OCR весь этот цикл не используется\n",
    "            for id_s, symbol in enumerate(symbols):\n",
    "                # TODO - убрать сохранение промежутоных файлов, они используются только для визуального контроля\n",
    "                word_dir = os.path.join(temp_dir, str(id_w))     # создаем очередную вложенную папку для котроля\n",
    "                if not os.path.exists(word_dir):\n",
    "                    os.mkdir(word_dir)\n",
    "\n",
    "                x, w, y, h = symbol['x'], symbol['w'], symbol['y'], symbol['h']\n",
    "                # cv2.rectangle(img_word, (x, y), (x+w, y+h), (70, 0, 0), 2) # для контрольной картинки\n",
    "                temp_symbol = size_cut_blocks(x=x, w=w, y=y, h=h, C=1.2) # получаем [['x1', 'x2', ...], 'w', 'y', 'h']\n",
    "                shape_y, shape_x = img_word.shape      # получаем размеры вырезанного слова, из которого извлекаем буквы\n",
    "                \n",
    "                for id_o, x_one_symbol in enumerate(temp_symbol[0]):\n",
    "                    x1 = x_one_symbol\n",
    "                    y1 = temp_symbol[2]\n",
    "                    x2 = x1 + temp_symbol[1]      # x2 = x + w\n",
    "                    y2 = y1 + temp_symbol[3]      # y2 = y + h\n",
    "                    x1, y1 = max(x1, 0), max(y1, 0)   # если 'x1' и 'y1' имеют отрицательное значение - приравниваем к 0\n",
    "                    x2 = min(x2, shape_x)   # если 'x2' больше, чем размер изображения - приравниваем к размеру изображения\n",
    "                    y2 = min(y2, shape_y)   # аналогично 'x2'\n",
    "                    one_symbol = img_word[y1:y2, x1:x2]\n",
    "                    #one_symbol = word_blocks[y1:y2, x1:x2]   # кидаем на распознавание контрастное изображение вместо паспорта\n",
    "                    #one_symbol = word_formatted[y1:y2, x1:x2]   # кидаем на распознавание контрастное изображение вместо паспорта\n",
    "                    # \n",
    "                    one_symbol = cv2.resize(one_symbol, (DATASET_SYMBOL_SIZE, DATASET_SYMBOL_SIZE))\n",
    "                    cv2.imwrite(os.path.join(word_dir, f'{id_s}-{id_o}.jpg'), one_symbol)\n",
    "                    \n",
    "                    # Основной вывод текста\n",
    "                    export_words[id_p][id_w].append(recognize_symbol(model=model, symbol_img=one_symbol))\n",
    "                    # cv2.rectangle(img_word, (x1, y1), (x2, y2), (0, 0, 150), 1) # для контрольной картинки\n",
    "            \n",
    "            cv2.imwrite(os.path.join(temp_dir, f'{id_w}_symbols.jpg'), img_word)   #сохраняем файлы только для контроля\n",
    "            #cv2.imwrite(os.path.join(temp_dir, f'{id_w}_blocks.jpg'), word_blocks)   #сохраняем файлы только для контроля\n",
    "            \n",
    "            # Временный вывод текста с распознаванием через Tesseract OCR\n",
    "            # recognized_text = recognize_by_tesseract(img_word)\n",
    "            #recognized_text = recognized_text.translate({ord(c): None for c in string.whitespace}) # удаляем лишние пробелы\n",
    "            #export_words[id_p][id_w].append(recognized_text)\n",
    "            \n",
    "                    \n",
    "    return export_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64a090ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pass_photos\\\\0.jpeg', 'pass_photos\\\\1.jpeg', 'pass_photos\\\\2.jpeg']\n"
     ]
    }
   ],
   "source": [
    "passports = get_files(WORK_DIR)\n",
    "print(passports[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8ff6fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model just created will be used.\n",
      "==== Image 0.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Ы\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Д\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "П\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ы\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Д\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "П\n",
      "==== Image 1.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Х\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "М\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "У\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "У\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Н\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "С\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "П\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "М\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "А\n",
      "==== Image 2.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ф\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Ф\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "П\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "К\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Д\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "К\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "П\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "К\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Д\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "В\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "А\n",
      "==== Image 3.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "М\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Д\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "К\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "С\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ч\n",
      "==== Image 4.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ц\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ю\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "М\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Ч\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ч\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Р\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "О\n",
      "==== Image 5.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ы\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ц\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ш\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Ю\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Р\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Х\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "М\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ф\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Р\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ы\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ы\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ы\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ч\n",
      "==== Image 6.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "С\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "С\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Х\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Р\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Р\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Р\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "П\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Р\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ч\n",
      "==== Image 7.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ы\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "К\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "П\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ф\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ф\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ы\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "К\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "П\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Ц\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Щ\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Н\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ч\n",
      "==== Image 8.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "К\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "У\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "К\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "М\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "О\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Е\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ч\n",
      "==== Image 9.jpg =====\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ш\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Щ\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "А\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "И\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Т\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Л\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Г\n",
      "\n",
      " (1, 28, 28)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Г\n"
     ]
    }
   ],
   "source": [
    "if 'model_name' in locals() or 'model_name' in globals():\n",
    "    print(\"The model just created will be used.\")\n",
    "else:\n",
    "    model_name = MODEL_PATH\n",
    "\n",
    "# TODO - тут потенциалная ошибка, если модель сохраняем в другом месте\n",
    "model = keras.models.load_model(model_name)\n",
    "symbols = passport_data_parser(work_dir=WORK_DIR, count=0, model=model)    # count=0 - значение по-умолчанию для распознавания всех\n",
    "#symbols.append(passport_data_parser(work_dir=WORK_DIR, count=5))   # Для теста разбираем только 1 паспорт\n",
    "#print(len(symbols[0]))\n",
    "\n",
    "result = ''\n",
    "for i_p, passport in enumerate(symbols):\n",
    "    name_second = ''.join(symbols[i_p][0])\n",
    "    name_first = ''.join(symbols[i_p][1])\n",
    "    name_middle = ''.join(symbols[i_p][2])\n",
    "    result += f'Паспорт: {i_p}:\\nФамилия: {name_second}\\nИмя: {name_first}\\nОтчество {name_middle}\\n\\n'\n",
    "\n",
    "with open ('task_3.txt', 'w') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7b30a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Паспорт: 0:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 1:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 2:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 3:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 4:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 5:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 6:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 7:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 8:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "Паспорт: 9:\n",
      "Фамилия: \n",
      "Имя: \n",
      "Отчество \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df56938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "\n",
    "def random_rotate_image(image):\n",
    "    image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n",
    "    return image\n",
    "\n",
    "image, label = next(iter(images_ds))\n",
    "image = random_rotate_image(image)\n",
    "show(image, label)\n",
    "\n",
    "def tf_random_rotate_image(image, label):\n",
    "    im_shape = image.shape\n",
    "    [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])\n",
    "    image.set_shape(im_shape)\n",
    "    return image, label\n",
    "\n",
    "rot_ds = images_ds.map(tf_random_rotate_image)\n",
    "\n",
    "for image, label in rot_ds.take(2):\n",
    "    show(image, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Способ применить функцию ко всем элементам датасета\n",
    "# Rebuild the flower filenames dataset:\n",
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
    "\n",
    "\n",
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def parse_image(filename):\n",
    "    parts = tf.strings.split(filename, os.sep)\n",
    "    label = parts[-2]\n",
    "\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [128, 128])\n",
    "    return image, label\n",
    "\n",
    "file_path = next(iter(list_ds))\n",
    "image, label = parse_image(file_path)\n",
    "\n",
    "# Test that it works.\n",
    "def show(image, label):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(label.numpy().decode('utf-8'))\n",
    "    plt.axis('off')\n",
    "\n",
    "show(image, label)\n",
    "\n",
    "# Map it over the dataset.\n",
    "images_ds = list_ds.map(parse_image)\n",
    "\n",
    "for image, label in images_ds.take(2):\n",
    "    show(image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c64b4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "img = cv2.imread(r\"D:\\work\\test_comp_vision\\test_for_MindSet\\pass_temp\\9_symbols.jpg\")\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# img_gamma = set_gamma_corr(image=img)\n",
    "#img_uint8 = image.img_to_array(img, dtype='uint8')\n",
    "#blur = cv2.medianBlur(img_uint8, 5)\n",
    "# TODO - закомиттить в доку описание ошибки CV_8UC1. adaptive Threshold требует на вход изгбражение строкго gray!\n",
    "# необходимо это описать либо в документации, либо в коде ошибки.\n",
    "# cv2  error: (-215:Assertion failed) src.type() == CV_8UC1 in function 'cv::adaptiveThreshold'\n",
    "tresh1 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 9, 5)\n",
    "#tresh2 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 9, 7)\n",
    "tresh3 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 9, 9)\n",
    "tresh4 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 9)\n",
    "tresh5 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 35, 9)\n",
    "tresh6 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 11)\n",
    "tresh7 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 25)  # ФАВОРИТ\n",
    "\n",
    "# cv2.imshow('Gamma 0.25', set_gamma_corr(image=img, gamma=0.25))\n",
    "# cv2.imshow('Gamma 0.5', set_gamma_corr(image=img, gamma=0.5))\n",
    "cv2.imshow('orig_image', img)\n",
    "# cv2.imshow('Gamma 2.0', set_gamma_corr(image=img, gamma=2.0))\n",
    "cv2.imshow('Contrast 1.5', cv2.convertScaleAbs(gray, alpha=1.5, beta=0))\n",
    "#cv2.imshow('Contrast 1.75', cv2.convertScaleAbs(gray, alpha=1.75, beta=0))\n",
    "cv2.imshow('TRESH 9, 5', tresh1)\n",
    "#cv2.imshow('TRESH 9, 7', tresh2)\n",
    "#cont = cv2.convertScaleAbs(gray, alpha=1.25, beta=0)\n",
    "tresh8 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 25, 11)\n",
    "cv2.imshow('TRESH 25, 11', tresh8)\n",
    "cv2.imshow('TRESH 25, 9', tresh4)\n",
    "cv2.imshow('TRESH 35, 9', tresh5)\n",
    "cv2.imshow('TRESH 11, 11', tresh6)\n",
    "cv2.imshow('TRESH 25, 25', tresh7)\n",
    "\n",
    "cv2.waitKey(0)  \n",
    "# cv2.destroyAllwindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b34a0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 255\n",
      "uint8\n",
      "float32\n",
      "-0.8179799 1.0078893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_test = cv2.imread(r\"D:\\work\\test_comp_vision\\test_for_MindSet\\pass_temp\\6_symbols.jpg\")\n",
    "image_test = cv2.cvtColor(image_test, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def normalize_it(x, min_fact, max_fact):\n",
    "    # function to apply for pixels value with vectorization \n",
    "    return((x - min_fact) / (max_fact - min_fact))\n",
    "\n",
    "\n",
    "# TODO - дополнить возможностью загрузки изображений по ссылке и объекта PIL.Image\n",
    "def normalize_img_color(image):\n",
    "    \"\"\"\n",
    "    Input: image as numpy.array() object - cv2.image. Strongly recommended GRAY\n",
    "    Output: image as numpy.array() object - cv2.image. Strongly recommended GRAY\n",
    "    Gets image as numpy.array() from cv2.Image and normalize values of pixels to [0.0-1.0] format.\n",
    "    That increase image contrast\n",
    "    Подсказка от GPTchat о том, как нормализовать значения к нужному диапазону.\n",
    "    (x - min) / (max - min)   # нормализуем изображение к диапазону [0.0-1.0]\n",
    "    x * (max - min) + min                   # преобразование нормального изображения в требуемому диапазону\n",
    "    \"\"\"\n",
    "    print(image.dtype)\n",
    "    image = image.astype(np.float32)\n",
    "    image /= 255.0\n",
    "    print(image.dtype)\n",
    "    blur = cv2.GaussianBlur(image, (15,15), 0)         # блерим для того, чтобы исключить шум\n",
    "    cv2.imshow('blur', blur)\n",
    "    \n",
    "    norm_vec = np.vectorize(normalize_it)     # Векторизуем функцию для нормализации массива\n",
    "    min_fact = blur.min()\n",
    "    max_fact = blur.max()\n",
    "    \n",
    "    return norm_vec(image, min_fact, max_fact)\n",
    "\n",
    "img_test_min, img_test_max = image_test.min(), image_test.max()\n",
    "print(img_test_min, img_test_max)\n",
    "\n",
    "image_norm = normalize_img_color(image_test)\n",
    "img_norm_min, img_norm_max = image_norm.min(), image_norm.max()\n",
    "print(img_norm_min, img_norm_max)\n",
    "\n",
    "\n",
    "cv2.imshow('original', image_test)\n",
    "cv2.imshow('NORMALIZED', image_norm)\n",
    "\n",
    "cv2.waitKey(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfcd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e03455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
